# Spark with Iceberg REST Catalog Configuration

# Iceberg Extensions
spark.sql.extensions                              org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Default Catalog
spark.sql.defaultCatalog                          lakehouse

# Lakehouse Catalog - Iceberg REST
spark.sql.catalog.lakehouse                       org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.lakehouse.type                  rest
spark.sql.catalog.lakehouse.uri                   http://iceberg-rest:8181
spark.sql.catalog.lakehouse.io-impl               org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.lakehouse.warehouse             s3://lakehouse/
spark.sql.catalog.lakehouse.s3.endpoint           http://minio:9000
spark.sql.catalog.lakehouse.s3.access-key-id      admin
spark.sql.catalog.lakehouse.s3.secret-access-key  password
spark.sql.catalog.lakehouse.s3.path-style-access  true

# S3A Hadoop Configuration (for reading/writing files)
spark.hadoop.fs.s3a.endpoint                      http://minio:9000
spark.hadoop.fs.s3a.access.key                    admin
spark.hadoop.fs.s3a.secret.key                    password
spark.hadoop.fs.s3a.path.style.access             true
spark.hadoop.fs.s3a.impl                          org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled        false
spark.hadoop.fs.s3a.aws.credentials.provider      org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Resource Configuration - Cluster Mode (Master + 2 Workers)
spark.master                                      spark://spark-master:7077

# Driver runs on Master node (4GB container limit)
spark.driver.memory                               2g
spark.driver.cores                                1

# Executor runs on Worker nodes
# NOTE: These are DEFAULTS for Jupyter notebooks
# Thrift Server has its own config in entrypoint-master.sh
spark.executor.memory                             2g
spark.executor.cores                              2
spark.executor.instances                          2

# Dynamic allocation - AUTO scale executors based on workload
spark.dynamicAllocation.enabled                   true
spark.dynamicAllocation.minExecutors              1
spark.dynamicAllocation.maxExecutors              4
spark.dynamicAllocation.initialExecutors          1
spark.dynamicAllocation.executorIdleTimeout       60s
spark.shuffle.service.enabled                     false

# Memory tuning
spark.memory.fraction                             0.8
spark.memory.storageFraction                      0.3

# Shuffle partitions - increase for distributed processing
spark.sql.shuffle.partitions                      32

# Network timeout for cluster communication
spark.network.timeout                             300s
spark.executor.heartbeatInterval                  60s

# Disable Event Logging (folder doesn't exist in tabulario image)
spark.eventLog.enabled                            false

# Performance tuning
spark.sql.adaptive.enabled                        true
spark.sql.adaptive.coalescePartitions.enabled     true
spark.sql.adaptive.skewJoin.enabled               true
