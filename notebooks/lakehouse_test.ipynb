{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# FULLSTACK OPEN-SOURCE LAKEHOUSE PLATFORM\n",
    "\n",
    "## E-Commerce Event History Analysis (Sử dụng bộ dữ liệu eCommerce Events History in Cosmetics Shop trên Kanggle)\n",
    "\n",
    "### Architecture Stack\n",
    "| Layer | Technology | Port |\n",
    "|-------|------------|------|\n",
    "| Storage | MinIO | 9000, 9001 |\n",
    "| Table Format | Apache Iceberg | - |\n",
    "| Catalog | Iceberg REST | 8181 |\n",
    "| Compute | Apache Spark | 8080, 8888 |\n",
    "| Transformation | dbt | - |\n",
    "| Serving | ClickHouse | 8123, 9440 |\n",
    "| Visualization | Apache Superset | 8088 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n",
      "Catalog: lakehouse\n",
      "Master: local[8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/30 03:53:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lakehouse Complete Demo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "namespaces",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Medallion namespaces\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.gold\")\n",
    "\n",
    "spark.sql(\"SHOW NAMESPACES IN lakehouse\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: BRONZE LAYER - Raw Data Ingestion\n",
    "\n",
    "- Read CSV raw data\n",
    "- Add metadata columns: `_ingestion_time`, `_source_file`\n",
    "- Write as Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bronze-read",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 3,533,286\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n",
      "+-------------------+----------------+----------+-------------------+-------------+---------+-----+---------+------------------------------------+\n",
      "|event_time         |event_type      |product_id|category_id        |category_code|brand    |price|user_id  |user_session                        |\n",
      "+-------------------+----------------+----------+-------------------+-------------+---------+-----+---------+------------------------------------+\n",
      "|2019-12-01 00:00:00|remove_from_cart|5712790   |1487580005268456287|NULL         |f.o.x    |6.27 |576802932|51d85cb0-897f-48d2-918b-ad63965c12dc|\n",
      "|2019-12-01 00:00:00|view            |5764655   |1487580005411062629|NULL         |cnd      |29.05|412120092|8adff31e-2051-4894-9758-224bfa8aec18|\n",
      "|2019-12-01 00:00:02|cart            |4958      |1487580009471148064|NULL         |runail   |1.19 |494077766|c99a50e8-2fac-4c4d-89ec-41c05f114554|\n",
      "|2019-12-01 00:00:05|view            |5848413   |1487580007675986893|NULL         |freedecor|0.79 |348405118|722ffea5-73c0-4924-8e8f-371ff8031af4|\n",
      "|2019-12-01 00:00:07|view            |5824148   |1487580005511725929|NULL         |NULL     |5.56 |576005683|28172809-7e4a-45ce-bab0-5efa90117cd5|\n",
      "+-------------------+----------------+----------+-------------------+-------------+---------+-----+---------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read raw CSV 2019-Dec\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Dec.csv\")\n",
    "\n",
    "print(f\"Total records: {df_raw.count():,}\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eddfa89-7ba2-40ee-888a-0758156cb3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 4,264,752\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+----------+-------------------+-------------+--------+-----+---------+------------------------------------+\n",
      "|event_time         |event_type|product_id|category_id        |category_code|brand   |price|user_id  |user_session                        |\n",
      "+-------------------+----------+----------+-------------------+-------------+--------+-----+---------+------------------------------------+\n",
      "|2020-01-01 00:00:00|view      |5809910   |1602943681873052386|NULL         |grattol |5.24 |595414620|4adb70bb-edbd-4981-b60f-a05bfd32683a|\n",
      "|2020-01-01 00:00:09|view      |5812943   |1487580012121948301|NULL         |kinetics|3.97 |595414640|c8c5205d-be43-4f1d-aa56-4828b8151c8a|\n",
      "|2020-01-01 00:00:19|view      |5798924   |1783999068867920626|NULL         |zinger  |3.97 |595412617|46a5010f-bd69-4fbe-a00d-bb17aa7b46f3|\n",
      "|2020-01-01 00:00:24|view      |5793052   |1487580005754995573|NULL         |NULL    |4.92 |420652863|546f6af3-a517-4752-a98b-80c4c5860711|\n",
      "|2020-01-01 00:00:25|view      |5899926   |2115334439910245200|NULL         |NULL    |3.92 |484071203|cff70ddf-529e-4b0c-a4fc-f43a749c0acb|\n",
      "+-------------------+----------+----------+-------------------+-------------+--------+-----+---------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read raw CSV 2019-Nov\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2020-Jan.csv\")\n",
    "\n",
    "print(f\"Total records: {df_raw.count():,}\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bronze-write",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRONZE layer created!\n",
      "+-------+\n",
      "|  total|\n",
      "+-------+\n",
      "|4264752|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add metadata columns\n",
    "df_bronze = df_raw \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2020-Jan.csv\"))\n",
    "\n",
    "# Write to Bronze layer\n",
    "df_bronze.writeTo(\"lakehouse.bronze.ecommerce_events\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"BRONZE layer created!\")\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM lakehouse.bronze.ecommerce_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: SILVER LAYER - Cleaned Data with Partitioning\n",
    "\n",
    "- Parse timestamps\n",
    "- Handle NULL values\n",
    "- **Partition by event_date** (optimize query performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "silver-transform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = false)\n",
      " |-- brand: string (nullable = false)\n",
      " |-- price: decimal(10,2) (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      " |-- _processed_at: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ******** Sửa withcolumn thành cast\n",
    "\n",
    "# Transform Bronze -> Silver\n",
    "df_silver = spark.table(\"lakehouse.bronze.ecommerce_events\") \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss 'UTC'\")) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"brand\", coalesce(col(\"brand\"), lit(\"unknown\"))) \\\n",
    "    .withColumn(\"category_code\", coalesce(col(\"category_code\"), lit(\"uncategorized\"))) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"decimal(10,2)\")) \\\n",
    "    .withColumn(\"_processed_at\", current_timestamp()) \\\n",
    "    .drop(\"event_time\", \"_ingestion_time\", \"_source_file\")\n",
    "\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "silver-write",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SILVER layer created with partitioning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_date|\n",
      "+----------+\n",
      "|2020-01-01|\n",
      "|2020-01-02|\n",
      "|2020-01-03|\n",
      "|2020-01-04|\n",
      "|2020-01-05|\n",
      "|2020-01-06|\n",
      "|2020-01-07|\n",
      "|2020-01-08|\n",
      "|2020-01-09|\n",
      "|2020-01-10|\n",
      "|2020-01-11|\n",
      "|2020-01-12|\n",
      "|2020-01-13|\n",
      "|2020-01-14|\n",
      "|2020-01-15|\n",
      "|2020-01-16|\n",
      "|2020-01-17|\n",
      "|2020-01-18|\n",
      "|2020-01-19|\n",
      "|2020-01-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write with PARTITIONING by date\n",
    "df_silver.writeTo(\"lakehouse.silver.ecommerce_events_cleaned\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"SILVER layer created with partitioning!\")\n",
    "\n",
    "# Verify partitions\n",
    "spark.sql(\"SELECT DISTINCT event_date FROM lakehouse.silver.ecommerce_events_cleaned ORDER BY event_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-evolution-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3.1: SCHEMA EVOLUTION Demo\n",
    "\n",
    "Simulate adding new column `payment_method` to source data.\n",
    "Iceberg handles schema changes automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "schema-evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'payment_method' added!\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|event_time     |timestamp|NULL   |\n",
      "|event_type     |string   |NULL   |\n",
      "|product_id     |int      |NULL   |\n",
      "|category_id    |bigint   |NULL   |\n",
      "|category_code  |string   |NULL   |\n",
      "|brand          |string   |NULL   |\n",
      "|price          |double   |NULL   |\n",
      "|user_id        |int      |NULL   |\n",
      "|user_session   |string   |NULL   |\n",
      "|_ingestion_time|timestamp|NULL   |\n",
      "|_source_file   |string   |NULL   |\n",
      "|payment_method |string   |NULL   |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column to Bronze table (Schema Evolution)\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE lakehouse.bronze.ecommerce_events \n",
    "    ADD COLUMN payment_method STRING\n",
    "\"\"\")\n",
    "\n",
    "print(\"Column 'payment_method' added!\")\n",
    "spark.sql(\"DESCRIBE lakehouse.bronze.ecommerce_events\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "schema-evolution-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+------+--------------+\n",
      "|event_time         |brand  |price |payment_method|\n",
      "+-------------------+-------+------+--------------+\n",
      "|2019-10-02 10:00:00|samsung|599.99|credit_card   |\n",
      "|2019-10-02 11:00:00|apple  |999.99|paypal        |\n",
      "+-------------------+-------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Insert new data with the new column (simulating Day T+1 data)\n",
    "from datetime import datetime\n",
    "\n",
    "new_data = [\n",
    "    (datetime(2019, 10, 2, 10, 0, 0), \"purchase\", 12345, 1234567890, \"electronics.phone\",\n",
    "     \"samsung\", 599.99, 100001, \"new-session-001\", datetime.now(), \"demo_day2.csv\", \"credit_card\"),\n",
    "\n",
    "    (datetime(2019, 10, 2, 11, 0, 0), \"purchase\", 12346, 1234567890, \"electronics.phone\",\n",
    "     \"apple\", 999.99, 100002, \"new-session-002\", datetime.now(), \"demo_day2.csv\", \"paypal\"),\n",
    "]\n",
    "\n",
    "\n",
    "schema = spark.table(\"lakehouse.bronze.ecommerce_events\").schema\n",
    "df_new = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "df_new.writeTo(\"lakehouse.bronze.ecommerce_events\").append()\n",
    "\n",
    "# Verify - old data has NULL for payment_method, new data has values\n",
    "spark.sql(\"\"\"\n",
    "    SELECT event_time, brand, price, payment_method \n",
    "    FROM lakehouse.bronze.ecommerce_events \n",
    "    WHERE payment_method IS NOT NULL OR brand = 'samsung'\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abfc66aa-6a9c-451d-a222-ba78728d746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('event_time', TimestampType(), True), StructField('event_type', StringType(), True), StructField('product_id', IntegerType(), True), StructField('category_id', LongType(), True), StructField('category_code', StringType(), True), StructField('brand', StringType(), True), StructField('price', DoubleType(), True), StructField('user_id', IntegerType(), True), StructField('user_session', StringType(), True), StructField('_ingestion_time', TimestampType(), True), StructField('_source_file', StringType(), True), StructField('payment_method', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-travel-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3.2: TIME TRAVEL Demo\n",
    "\n",
    "Query historical snapshots of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "time-travel-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TABLE HISTORY ===\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-30 04:10:44.685|9127348836860691094|NULL               |true               |\n",
      "|2026-01-30 04:19:26.313|6471009077848248100|9127348836860691094|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "print(\"=== TABLE HISTORY ===\")\n",
    "spark.sql(\"SELECT * FROM lakehouse.bronze.ecommerce_events.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "time-travel-snapshots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SNAPSHOTS ===\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|8345581712853693656|2026-01-30 04:04:51.168|append   |\n",
      "|8961319236875262264|2026-01-30 04:08:08.245|append   |\n",
      "|7909682360207467037|2026-01-30 04:09:21.43 |append   |\n",
      "|4160572160808099697|2026-01-30 04:10:06.081|append   |\n",
      "|9127348836860691094|2026-01-30 04:10:44.685|append   |\n",
      "|6471009077848248100|2026-01-30 04:19:26.313|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "First snapshot ID: 8345581712853693656\n"
     ]
    }
   ],
   "source": [
    "# View snapshots\n",
    "print(\"=== SNAPSHOTS ===\")\n",
    "snapshots_df = spark.sql(\"SELECT snapshot_id, committed_at, operation FROM lakehouse.bronze.ecommerce_events.snapshots\")\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "# Get first snapshot ID for time travel\n",
    "first_snapshot = snapshots_df.orderBy(\"committed_at\").first()[\"snapshot_id\"]\n",
    "print(f\"First snapshot ID: {first_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "time-travel-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA AT FIRST SNAPSHOT (before new column) ===\n",
      "+-----------------+\n",
      "|count_at_snapshot|\n",
      "+-----------------+\n",
      "|          3533286|\n",
      "+-----------------+\n",
      "\n",
      "=== CURRENT DATA (after inserts) ===\n",
      "+-------------+\n",
      "|current_count|\n",
      "+-------------+\n",
      "|      4264754|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cột count_at_snapshot là gì\n",
    "\n",
    "# Query data at first snapshot (before schema evolution)\n",
    "print(\"=== DATA AT FIRST SNAPSHOT (before new column) ===\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count_at_snapshot   \n",
    "    FROM lakehouse.bronze.ecommerce_events \n",
    "    VERSION AS OF {first_snapshot}\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"=== CURRENT DATA (after inserts) ===\")\n",
    "spark.sql(\"SELECT COUNT(*) as current_count FROM lakehouse.bronze.ecommerce_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: GOLD LAYER - Aggregated Tables\n",
    "\n",
    "Create business-level aggregations with **partitioning** and **sorting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gold-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: daily_sales_summary created!\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "|event_date|total_events|views|add_to_cart|purchases| revenue|unique_users|conversion_rate|      _aggregated_at|\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "|2020-01-19|      137205|65062|      36127|     8168|39367.78|       18115|          12.55|2026-01-30 04:32:...|\n",
      "|2020-01-20|      152557|72900|      40810|     9014|45517.88|       20310|          12.36|2026-01-30 04:32:...|\n",
      "|2020-01-17|      140112|65064|      37400|     9265|43294.43|       18647|          14.24|2026-01-30 04:32:...|\n",
      "|2020-01-18|      119252|57418|      32316|     6197|32230.27|       16698|          10.79|2026-01-30 04:32:...|\n",
      "|2020-01-23|      141652|69529|      38188|     8700|45708.89|       20111|          12.51|2026-01-30 04:32:...|\n",
      "|2020-01-24|      135184|66086|      35532|     8779|42814.01|       19012|          13.28|2026-01-30 04:32:...|\n",
      "|2020-01-21|      150404|70681|      42499|     9165|47214.73|       20778|          12.97|2026-01-30 04:32:...|\n",
      "|2020-01-22|      138344|66255|      37601|     9088|44715.82|       19811|          13.72|2026-01-30 04:32:...|\n",
      "|2020-01-27|      206611|88661|      56506|    18314|91419.66|       22141|          20.66|2026-01-30 04:32:...|\n",
      "|2020-01-28|      184379|81976|      49087|    17352|87725.65|       21143|          21.17|2026-01-30 04:32:...|\n",
      "|2020-01-25|      120436|58142|      31808|     6766|32850.71|       17643|          11.64|2026-01-30 04:32:...|\n",
      "|2020-01-26|      134735|65300|      35319|     8182|39995.78|       19012|          12.53|2026-01-30 04:32:...|\n",
      "|2020-01-31|      132564|66223|      36017|     7346|36196.44|       21620|          11.09|2026-01-30 04:32:...|\n",
      "|2020-01-29|      145257|70952|      40400|     7605|39372.25|       21740|          10.72|2026-01-30 04:32:...|\n",
      "|2020-01-30|      142843|68605|      39510|     7582|38993.52|       21366|          11.05|2026-01-30 04:32:...|\n",
      "|2020-01-03|      116893|55440|      31022|     6031|32840.06|       16125|          10.88|2026-01-30 04:32:...|\n",
      "|2020-01-04|      116617|55937|      30756|     6602|32513.20|       16797|          11.80|2026-01-30 04:32:...|\n",
      "|2020-01-01|       80940|39631|      21874|     3269|16765.95|       12296|           8.25|2026-01-30 04:32:...|\n",
      "|2020-01-02|      103157|49914|      27823|     4875|25067.09|       14682|           9.77|2026-01-30 04:32:...|\n",
      "|2020-01-07|      125209|59653|      34096|     7564|38830.28|       17845|          12.68|2026-01-30 04:32:...|\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cần viết lại bằng spark\n",
    "# chỗ agg_at chuyển như nào?? \n",
    "\n",
    "# Gold 1: Daily Sales Summary (with partitioning)\n",
    "df_daily = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'cart' THEN 1 END) AS add_to_cart,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        COUNT(DISTINCT user_id) AS unique_users,\n",
    "        ROUND(COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) * 100.0 / \n",
    "              NULLIF(COUNT(CASE WHEN event_type = 'view' THEN 1 END), 0), 2) AS conversion_rate,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    GROUP BY event_date\n",
    "    ORDER BY event_date\n",
    "\"\"\")\n",
    "\n",
    "df_daily.writeTo(\"lakehouse.gold.daily_sales_summary\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"Gold: daily_sales_summary created!\")\n",
    "spark.table(\"lakehouse.gold.daily_sales_summary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "gold-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: brand_performance created!\n",
      "+--------+------------+------+---------+--------+---------------+----------------+--------------------+\n",
      "|   brand|total_events| views|purchases| revenue|avg_order_value|unique_customers|      _aggregated_at|\n",
      "+--------+------------+------+---------+--------+---------------+----------------+--------------------+\n",
      "|  runail|      368018|144126|    26596|80707.34|           3.03|           59595|2026-01-30 04:36:...|\n",
      "| grattol|      193829| 97695|    11828|63743.67|           5.39|           34884|2026-01-30 04:36:...|\n",
      "|   irisk|      210875| 87235|    14827|45746.89|           3.09|           48028|2026-01-30 04:36:...|\n",
      "|     uno|       53625| 26302|     3811|42533.50|          11.16|           16370|2026-01-30 04:36:...|\n",
      "|  strong|       13456| 11039|      199|34607.65|         173.91|            5683|2026-01-30 04:36:...|\n",
      "|  masura|      203526| 77498|    12234|34448.60|           2.82|           22668|2026-01-30 04:36:...|\n",
      "|   estel|       82032| 46753|     4504|26986.40|           5.99|           24438|2026-01-30 04:36:...|\n",
      "|jessnail|       54523| 40395|     1706|25641.09|          15.03|           21039|2026-01-30 04:36:...|\n",
      "|ingarden|       78388| 34675|     5156|25367.33|           4.92|           17078|2026-01-30 04:36:...|\n",
      "| italwax|       45063| 21986|     3455|20373.97|           5.90|           11600|2026-01-30 04:36:...|\n",
      "+--------+------------+------+---------+--------+---------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gold 2: Brand Performance (sorted for efficient queries)\n",
    "df_brand = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        brand,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        CAST(AVG(CASE WHEN event_type = 'purchase' THEN price END) AS DECIMAL(10,2)) AS avg_order_value,\n",
    "        COUNT(DISTINCT user_id) AS unique_customers,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    WHERE brand != 'unknown'\n",
    "    GROUP BY brand\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Write with sort order for optimized queries\n",
    "df_brand.writeTo(\"lakehouse.gold.brand_performance\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.distribution-mode\", \"hash\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"Gold: brand_performance created!\")\n",
    "spark.table(\"lakehouse.gold.brand_performance\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "gold-hourly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: hourly_traffic created!\n",
      "+----------+------------+------+---------+--------+--------------------+\n",
      "|event_hour|total_events| views|purchases| revenue|      _aggregated_at|\n",
      "+----------+------------+------+---------+--------+--------------------+\n",
      "|         0|       44655| 20942|     2545|11884.37|2026-01-30 04:37:...|\n",
      "|         1|       39235| 16947|     2424|10916.11|2026-01-30 04:37:...|\n",
      "|         2|       42076| 17827|     2752|12891.20|2026-01-30 04:37:...|\n",
      "|         3|       54551| 24805|     2993|16182.73|2026-01-30 04:37:...|\n",
      "|         4|       73431| 35004|     3766|19599.12|2026-01-30 04:37:...|\n",
      "|         5|      111233| 51779|     6439|30018.33|2026-01-30 04:37:...|\n",
      "|         6|      148987| 70897|     9086|47735.20|2026-01-30 04:37:...|\n",
      "|         7|      182336| 88221|    11073|57550.47|2026-01-30 04:37:...|\n",
      "|         8|      204989| 98218|    13784|72978.64|2026-01-30 04:37:...|\n",
      "|         9|      228262|108271|    16230|82977.64|2026-01-30 04:37:...|\n",
      "|        10|      235303|112871|    16301|86686.52|2026-01-30 04:37:...|\n",
      "|        11|      247770|117353|    17088|89530.61|2026-01-30 04:37:...|\n",
      "|        12|      257423|121021|    17509|85957.43|2026-01-30 04:37:...|\n",
      "|        13|      245100|118792|    16604|81969.77|2026-01-30 04:37:...|\n",
      "|        14|      230020|110613|    14692|72267.56|2026-01-30 04:37:...|\n",
      "|        15|      226796|107425|    14282|71957.41|2026-01-30 04:37:...|\n",
      "|        16|      233746|111580|    13436|68107.50|2026-01-30 04:37:...|\n",
      "|        17|      251553|122659|    14269|71734.54|2026-01-30 04:37:...|\n",
      "|        18|      279249|133713|    15090|75292.66|2026-01-30 04:37:...|\n",
      "|        19|      290341|140099|    16670|78955.21|2026-01-30 04:37:...|\n",
      "|        20|      268372|130108|    15556|74247.99|2026-01-30 04:37:...|\n",
      "|        21|      185069| 88737|    10898|54096.27|2026-01-30 04:37:...|\n",
      "|        22|      115245| 55981|     5943|28406.11|2026-01-30 04:37:...|\n",
      "|        23|       69010| 33745|     4367|19592.09|2026-01-30 04:37:...|\n",
      "+----------+------------+------+---------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tại sao truy vấn này lại nhanh hơn trên??\n",
    "# Gold 3: Hourly Traffic Pattern\n",
    "df_hourly = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_hour,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    GROUP BY event_hour\n",
    "    ORDER BY event_hour\n",
    "\"\"\")\n",
    "\n",
    "df_hourly.writeTo(\"lakehouse.gold.hourly_traffic\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"Gold: hourly_traffic created!\")\n",
    "spark.table(\"lakehouse.gold.hourly_traffic\").show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "gold-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: daily_sales_by_category created!\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "|event_date|       category_code|total_events|purchases|revenue|      _aggregated_at|\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "|2020-01-19|appliances.enviro...|         897|       28| 741.99|2026-01-30 04:40:...|\n",
      "|2020-01-19|furniture.bathroo...|         171|       10| 292.39|2026-01-30 04:40:...|\n",
      "|2020-01-19|       apparel.glove|         409|       34| 226.31|2026-01-30 04:40:...|\n",
      "|2020-01-19|appliances.person...|          38|        2| 101.58|2026-01-30 04:40:...|\n",
      "|2020-01-19| stationery.cartrige|         356|       32|  69.07|2026-01-30 04:40:...|\n",
      "|2020-01-19|     accessories.bag|         149|        1|  33.33|2026-01-30 04:40:...|\n",
      "|2020-01-19|accessories.cosme...|          15|        2|   9.54|2026-01-30 04:40:...|\n",
      "|2020-01-19|appliances.enviro...|          10|        1|   7.38|2026-01-30 04:40:...|\n",
      "|2020-01-19|furniture.living_...|           1|        0|   0.00|2026-01-30 04:40:...|\n",
      "|2020-01-19|furniture.living_...|         162|        0|   0.00|2026-01-30 04:40:...|\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gold 4: Daily Sales by Category\n",
    "df_category = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        category_code,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    WHERE category_code != 'uncategorized'\n",
    "    GROUP BY event_date, category_code\n",
    "    ORDER BY event_date, revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "df_category.writeTo(\"lakehouse.gold.daily_sales_by_category\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"Gold: daily_sales_by_category created!\")\n",
    "spark.table(\"lakehouse.gold.daily_sales_by_category\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4.1: Verify All Tables & Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "verify-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BRONZE TABLES\n",
      "============================================================\n",
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|   bronze|ecommerce_events|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n",
      "============================================================\n",
      "SILVER TABLES\n",
      "============================================================\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|   silver|ecommerce_events_...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "============================================================\n",
      "GOLD TABLES\n",
      "============================================================\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|     gold|   brand_performance|      false|\n",
      "|     gold|daily_sales_by_ca...|      false|\n",
      "|     gold| daily_sales_summary|      false|\n",
      "|     gold|      hourly_traffic|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BRONZE TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.bronze\").show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SILVER TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.silver\").show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GOLD TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.gold\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "verify-locations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Locations in MinIO:\n",
      "================================================================================\n",
      "lakehouse.bronze.ecommerce_events\n",
      "  -> s3://lakehouse/bronze/ecommerce_events\n",
      "lakehouse.silver.ecommerce_events_cleaned\n",
      "  -> s3://lakehouse/silver/ecommerce_events_cleaned\n",
      "lakehouse.gold.daily_sales_summary\n",
      "  -> s3://lakehouse/gold/daily_sales_summary\n",
      "lakehouse.gold.brand_performance\n",
      "  -> s3://lakehouse/gold/brand_performance\n",
      "lakehouse.gold.hourly_traffic\n",
      "  -> s3://lakehouse/gold/hourly_traffic\n",
      "lakehouse.gold.daily_sales_by_category\n",
      "  -> s3://lakehouse/gold/daily_sales_by_category\n"
     ]
    }
   ],
   "source": [
    "# Check file locations in MinIO\n",
    "tables = [\n",
    "    \"lakehouse.bronze.ecommerce_events\",\n",
    "    \"lakehouse.silver.ecommerce_events_cleaned\",\n",
    "    \"lakehouse.gold.daily_sales_summary\",\n",
    "    \"lakehouse.gold.brand_performance\",\n",
    "    \"lakehouse.gold.hourly_traffic\",\n",
    "    \"lakehouse.gold.daily_sales_by_category\"\n",
    "]\n",
    "\n",
    "print(\"Table Locations in MinIO:\")\n",
    "print(\"=\" * 80)\n",
    "for table in tables:\n",
    "    try:\n",
    "        location = spark.sql(f\"DESCRIBE EXTENDED {table}\") \\\n",
    "            .filter(col(\"col_name\") == \"Location\") \\\n",
    "            .select(\"data_type\").collect()[0][0]\n",
    "        print(f\"{table}\")\n",
    "        print(f\"  -> {location}\")\n",
    "    except:\n",
    "        print(f\"{table} - Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5: CLICKHOUSE Integration (Zero-Copy)\n",
    "\n",
    "### Run these commands in ClickHouse:\n",
    "```bash\n",
    "docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
    "```\n",
    "After that, run code  below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf511e23-6216-4eaa-a376-121601fae909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Có cần thiết phải qua clickhouse ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "clickhouse-commands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- ================================================================\n",
      "-- CLICKHOUSE COMMANDS - Run in clickhouse-client\n",
      "-- docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
      "-- ================================================================\n",
      "\n",
      "-- 1. Create database\n",
      "CREATE DATABASE IF NOT EXISTS lakehouse;\n",
      "\n",
      "-- 2. Daily Sales Summary (Zero-Copy from MinIO)\n",
      "CREATE OR REPLACE TABLE lakehouse.daily_sales_summary\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_summary/data/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "-- 3. Brand Performance\n",
      "CREATE OR REPLACE TABLE lakehouse.brand_performance\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/brand_performance/data/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "-- 4. Hourly Traffic\n",
      "CREATE OR REPLACE TABLE lakehouse.hourly_traffic\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/hourly_traffic/data/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "-- 5. Daily Sales by Category\n",
      "CREATE OR REPLACE TABLE lakehouse.daily_sales_by_category\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_by_category/data/*/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "\n",
      "-- ================================================================\n",
      "-- VERIFY DATA\n",
      "-- ================================================================\n",
      "\n",
      "SELECT * FROM lakehouse.daily_sales_summary;\n",
      "SELECT * FROM lakehouse.brand_performance LIMIT 10;\n",
      "SELECT * FROM lakehouse.hourly_traffic ORDER BY event_hour;\n",
      "\n",
      "-- ================================================================\n",
      "-- SAMPLE ANALYTICS QUERIES\n",
      "-- ================================================================\n",
      "\n",
      "-- Top 10 brands by revenue\n",
      "SELECT \n",
      "    brand,\n",
      "    revenue,\n",
      "    purchases,\n",
      "    round(revenue / purchases, 2) as avg_order_value\n",
      "FROM lakehouse.brand_performance\n",
      "WHERE purchases > 0\n",
      "ORDER BY revenue DESC\n",
      "LIMIT 10;\n",
      "\n",
      "-- Conversion funnel\n",
      "SELECT\n",
      "    sum(views) as total_views,\n",
      "    sum(add_to_cart) as total_carts,\n",
      "    sum(purchases) as total_purchases,\n",
      "    round(sum(purchases) * 100.0 / sum(views), 2) as overall_conversion\n",
      "FROM lakehouse.daily_sales_summary;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickhouse_sql = \"\"\"\n",
    "-- ================================================================\n",
    "-- CLICKHOUSE COMMANDS - Run in clickhouse-client\n",
    "-- docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
    "-- ================================================================\n",
    "\n",
    "-- 1. Create database\n",
    "CREATE DATABASE IF NOT EXISTS lakehouse;\n",
    "\n",
    "-- 2. Daily Sales Summary (Zero-Copy from MinIO)\n",
    "CREATE OR REPLACE TABLE lakehouse.daily_sales_summary\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_summary/data/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "-- 3. Brand Performance\n",
    "CREATE OR REPLACE TABLE lakehouse.brand_performance\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/brand_performance/data/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "-- 4. Hourly Traffic\n",
    "CREATE OR REPLACE TABLE lakehouse.hourly_traffic\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/hourly_traffic/data/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "-- 5. Daily Sales by Category\n",
    "CREATE OR REPLACE TABLE lakehouse.daily_sales_by_category\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_by_category/data/*/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "\n",
    "-- ================================================================\n",
    "-- VERIFY DATA\n",
    "-- ================================================================\n",
    "\n",
    "SELECT * FROM lakehouse.daily_sales_summary;\n",
    "SELECT * FROM lakehouse.brand_performance LIMIT 10;\n",
    "SELECT * FROM lakehouse.hourly_traffic ORDER BY event_hour;\n",
    "\n",
    "-- ================================================================\n",
    "-- SAMPLE ANALYTICS QUERIES\n",
    "-- ================================================================\n",
    "\n",
    "-- Top 10 brands by revenue\n",
    "SELECT \n",
    "    brand,\n",
    "    revenue,\n",
    "    purchases,\n",
    "    round(revenue / purchases, 2) as avg_order_value\n",
    "FROM lakehouse.brand_performance\n",
    "WHERE purchases > 0\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Conversion funnel\n",
    "SELECT\n",
    "    sum(views) as total_views,\n",
    "    sum(add_to_cart) as total_carts,\n",
    "    sum(purchases) as total_purchases,\n",
    "    round(sum(purchases) * 100.0 / sum(views), 2) as overall_conversion\n",
    "FROM lakehouse.daily_sales_summary;\n",
    "\"\"\"\n",
    "\n",
    "print(clickhouse_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superset-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 6: SUPERSET Dashboard Setup\n",
    "\n",
    "### Access\n",
    "- URL: http://localhost:8088\n",
    "- Username: `admin`\n",
    "- Password: `admin`\n",
    "\n",
    "### Add ClickHouse Connection\n",
    "1. Settings -> Database Connections -> + Database\n",
    "2. Select: **ClickHouse Connect**\n",
    "3. Connection :\n",
    "```\n",
    "-Host: clickhouse\n",
    "\n",
    "-Port: 8123\n",
    "\n",
    "-Database name: lakehouse\n",
    "\n",
    "-Username: admin\n",
    "\n",
    "-Password: password\n",
    "\n",
    "-Display Name: ClickHouse Lakehouse\n",
    "\n",
    "-SSL: (optional)\n",
    "\n",
    "click Connect/Test Connection.\n",
    "```\n",
    "\n",
    "### Create 3 Required Charts\n",
    "\n",
    "| Chart | Type | Dataset | Metrics |\n",
    "|-------|------|---------|--------|\n",
    "| Daily Revenue Trend | Line Chart | daily_sales_summary | revenue by event_date |\n",
    "| Top Brands | Bar Chart | brand_performance | revenue by brand |\n",
    "| Hourly Traffic | Area Chart | hourly_traffic | views, purchases by event_hour |\n",
    "\n",
    "### Create Dashboard\n",
    "1. Dashboards -> + Dashboard\n",
    "2. Add the 3 charts\n",
    "3. Arrange and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbt-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 7: dbt Transformation (Alternative)\n",
    "\n",
    "### Run dbt commands:\n",
    "```bash\n",
    "# Enter dbt container\n",
    "docker exec -it lakehouse-dbt bash\n",
    "\n",
    "# Run transformations\n",
    "cd /usr/app/dbt/lakehouse_dbt\n",
    "dbt debug\n",
    "dbt run\n",
    "```\n",
    "\n",
    "dbt models are located in:\n",
    "- `/dbt/lakehouse_dbt/models/silver/` - Silver layer transformations\n",
    "- `/dbt/lakehouse_dbt/models/gold/` - Gold layer aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e12560-fa85-4850-bc23-d65b57018dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
