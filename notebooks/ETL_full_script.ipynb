{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# FULLSTACK OPEN-SOURCE LAKEHOUSE PLATFORM\n",
    "\n",
    "## E-Commerce Event History Analysis (Sử dụng bộ dữ liệu eCommerce Events History in Cosmetics Shop trên Kanggle)\n",
    "\n",
    "### Architecture Stack\n",
    "| Layer | Technology | Port |\n",
    "|-------|------------|------|\n",
    "| Storage | MinIO | 9000, 9001 |\n",
    "| Table Format | Apache Iceberg | - |\n",
    "| Catalog | Iceberg REST | 8181 |\n",
    "| Compute | Apache Spark | 8080, 8888 |\n",
    "| Transformation | dbt | - |\n",
    "| Serving | ClickHouse | 8123, 9440 |\n",
    "| Visualization | Apache Superset | 8088 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/03 09:39:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/03 09:39:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Started new sparksession.\n",
      "Spark Version: 3.5.1\n",
      "Catalog: lakehouse\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✅ Stop previous SparkSession.\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lakehouse ETL eCommerces data\") \\\n",
    "    .getOrCreate()\n",
    "print(\"✅Started new sparksession.\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "namespaces",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Medallion namespaces\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.gold\")\n",
    "\n",
    "spark.sql(\"SHOW NAMESPACES IN lakehouse\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: BRONZE LAYER - Raw Data Ingestion\n",
    "\n",
    "- Read CSV raw data\n",
    "- Add metadata columns: `_ingestion_time`, `_source_file`\n",
    "- Write as Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bronze-read",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 39.01 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 20,692,840\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+------------------------------------+\n",
      "|event_time         |event_type|product_id|category_id        |category_code|brand |price|user_id  |user_session                        |\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+------------------------------------+\n",
      "|2019-10-01 00:00:00|cart      |5773203   |1487580005134238553|NULL         |runail|2.62 |463240011|26dd6e6e-4dac-4778-8d2c-92e149dab885|\n",
      "|2019-10-01 00:00:03|cart      |5773353   |1487580005134238553|NULL         |runail|2.62 |463240011|26dd6e6e-4dac-4778-8d2c-92e149dab885|\n",
      "|2019-10-01 00:00:07|cart      |5881589   |2151191071051219817|NULL         |lovely|13.48|429681830|49e8d843-adf3-428b-a2c3-fe8bc6a307c9|\n",
      "|2019-10-01 00:00:07|cart      |5723490   |1487580005134238553|NULL         |runail|2.62 |463240011|26dd6e6e-4dac-4778-8d2c-92e149dab885|\n",
      "|2019-10-01 00:00:15|cart      |5881449   |1487580013522845895|NULL         |lovely|0.56 |429681830|49e8d843-adf3-428b-a2c3-fe8bc6a307c9|\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_19_oct = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Oct.csv\")\n",
    "df_19_nov = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Nov.csv\")\n",
    "df_19_dec = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Dec.csv\")\n",
    "df_20_jan = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2020-Jan.csv\")\n",
    "df_20_feb = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2020-Feb.csv\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "\n",
    "df_raw = df_19_oct.union(df_19_nov).union(df_19_dec).union(df_20_jan).union(df_20_feb)\n",
    "\n",
    "print(f\"Total records: {df_raw.count():,}\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162863e-70e6-40c2-8b69-09bb8d959876",
   "metadata": {},
   "source": [
    "# Add metadata columns\n",
    "df_19_oct = df_19_oct \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2019-Oct.csv\"))\n",
    "df_19_nov = df_19_nov \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2019-Nov.csv\"))\n",
    "df_19_dec = df_19_dec \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2019-Dec.csv\"))\n",
    "df_20_jan = df_20_jan \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2020-Jan.csv\"))\n",
    "df_20_feb = df_20_feb \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2020-Feb.csv\"))\n",
    "\n",
    "df_bronze = df_19_oct.union(df_19_nov).union(df_19_dec).union(df_20_jan).union(df_20_feb)\n",
    "\n",
    "start = time.time()\n",
    "# Write to Bronze layer\n",
    "df_bronze.writeTo(\"lakehouse.bronze.ecommerce_events\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "\n",
    "print(\"BRONZE layer created!\")\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM lakehouse.bronze.ecommerce_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: SILVER LAYER - Cleaned Data with Partitioning\n",
    "\n",
    "- Parse timestamps\n",
    "- Handle NULL values\n",
    "- **Partition by event_date** (optimize query performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "silver-transform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 0.20 giây\n",
      "root\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = false)\n",
      " |-- brand: string (nullable = false)\n",
      " |-- price: decimal(10,2) (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      " |-- _processed_at: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Transform Bronze -> Silver\n",
    "df_silver = spark.table(\"lakehouse.bronze.ecommerce_events\") \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss 'UTC'\")) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"brand\", coalesce(col(\"brand\"), lit(\"unknown\"))) \\\n",
    "    .withColumn(\"category_code\", coalesce(col(\"category_code\"), lit(\"uncategorized\"))) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"decimal(10,2)\")) \\\n",
    "    .withColumn(\"_processed_at\", current_timestamp()) \\\n",
    "    .drop(\"event_time\", \"_ingestion_time\", \"_source_file\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "silver-write",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 42.20 giây\n",
      "SILVER layer created with partitioning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:======================================================> (37 + 1) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_date|\n",
      "+----------+\n",
      "|2019-10-01|\n",
      "|2019-10-02|\n",
      "|2019-10-03|\n",
      "|2019-10-04|\n",
      "|2019-10-05|\n",
      "|2019-10-06|\n",
      "|2019-10-07|\n",
      "|2019-10-08|\n",
      "|2019-10-09|\n",
      "|2019-10-10|\n",
      "|2019-10-11|\n",
      "|2019-10-12|\n",
      "|2019-10-13|\n",
      "|2019-10-14|\n",
      "|2019-10-15|\n",
      "|2019-10-16|\n",
      "|2019-10-17|\n",
      "|2019-10-18|\n",
      "|2019-10-19|\n",
      "|2019-10-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Write with PARTITIONING by date\n",
    "df_silver.writeTo(\"lakehouse.silver.ecommerce_events_cleaned\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "print(\"SILVER layer created with partitioning!\")\n",
    "\n",
    "# Verify partitions\n",
    "spark.sql(\"SELECT DISTINCT event_date FROM lakehouse.silver.ecommerce_events_cleaned ORDER BY event_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-evolution-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3.1: SCHEMA EVOLUTION Demo\n",
    "\n",
    "Simulate adding new column `payment_method` to source data.\n",
    "Iceberg handles schema changes automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "schema-evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'payment_method' added!\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|event_time     |timestamp|NULL   |\n",
      "|event_type     |string   |NULL   |\n",
      "|product_id     |int      |NULL   |\n",
      "|category_id    |bigint   |NULL   |\n",
      "|category_code  |string   |NULL   |\n",
      "|brand          |string   |NULL   |\n",
      "|price          |double   |NULL   |\n",
      "|user_id        |int      |NULL   |\n",
      "|user_session   |string   |NULL   |\n",
      "|_ingestion_time|timestamp|NULL   |\n",
      "|_source_file   |string   |NULL   |\n",
      "|payment_method |string   |NULL   |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column to Bronze table (Schema Evolution)\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE lakehouse.bronze.ecommerce_events \n",
    "    ADD COLUMN payment_method STRING\n",
    "\"\"\")\n",
    "\n",
    "print(\"Column 'payment_method' added!\")\n",
    "spark.sql(\"DESCRIBE lakehouse.bronze.ecommerce_events\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "schema-evolution-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+------+--------------+\n",
      "|event_time         |brand  |price |payment_method|\n",
      "+-------------------+-------+------+--------------+\n",
      "|2019-10-02 10:00:00|samsung|599.99|credit_card   |\n",
      "|2019-10-02 11:00:00|apple  |999.99|paypal        |\n",
      "+-------------------+-------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Insert new data with the new column (simulating Day T+1 data)\n",
    "from datetime import datetime\n",
    "\n",
    "new_data = [\n",
    "    (datetime(2019, 10, 2, 10, 0, 0), \"purchase\", 12345, 1234567890, \"electronics.phone\",\n",
    "     \"samsung\", 599.99, 100001, \"new-session-001\", datetime.now(), \"demo_day2.csv\", \"credit_card\"),\n",
    "\n",
    "    (datetime(2019, 10, 2, 11, 0, 0), \"purchase\", 12346, 1234567890, \"electronics.phone\",\n",
    "     \"apple\", 999.99, 100002, \"new-session-002\", datetime.now(), \"demo_day2.csv\", \"paypal\"),\n",
    "]\n",
    "\n",
    "\n",
    "schema = spark.table(\"lakehouse.bronze.ecommerce_events\").schema\n",
    "df_new = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "df_new.writeTo(\"lakehouse.bronze.ecommerce_events\").append()\n",
    "\n",
    "# Verify - old data has NULL for payment_method, new data has values\n",
    "spark.sql(\"\"\"\n",
    "    SELECT event_time, brand, price, payment_method \n",
    "    FROM lakehouse.bronze.ecommerce_events \n",
    "    WHERE payment_method IS NOT NULL OR brand = 'samsung'\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abfc66aa-6a9c-451d-a222-ba78728d746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('event_time', TimestampType(), True), StructField('event_type', StringType(), True), StructField('product_id', IntegerType(), True), StructField('category_id', LongType(), True), StructField('category_code', StringType(), True), StructField('brand', StringType(), True), StructField('price', DoubleType(), True), StructField('user_id', IntegerType(), True), StructField('user_session', StringType(), True), StructField('_ingestion_time', TimestampType(), True), StructField('_source_file', StringType(), True), StructField('payment_method', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-travel-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3.2: TIME TRAVEL Demo\n",
    "\n",
    "Query historical snapshots of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "time-travel-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TABLE HISTORY ===\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-30 04:10:44.685|9127348836860691094|NULL               |true               |\n",
      "|2026-01-30 04:19:26.313|6471009077848248100|9127348836860691094|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "print(\"=== TABLE HISTORY ===\")\n",
    "spark.sql(\"SELECT * FROM lakehouse.bronze.ecommerce_events.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "time-travel-snapshots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SNAPSHOTS ===\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|8345581712853693656|2026-01-30 04:04:51.168|append   |\n",
      "|8961319236875262264|2026-01-30 04:08:08.245|append   |\n",
      "|7909682360207467037|2026-01-30 04:09:21.43 |append   |\n",
      "|4160572160808099697|2026-01-30 04:10:06.081|append   |\n",
      "|9127348836860691094|2026-01-30 04:10:44.685|append   |\n",
      "|6471009077848248100|2026-01-30 04:19:26.313|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "First snapshot ID: 8345581712853693656\n"
     ]
    }
   ],
   "source": [
    "# View snapshots\n",
    "print(\"=== SNAPSHOTS ===\")\n",
    "snapshots_df = spark.sql(\"SELECT snapshot_id, committed_at, operation FROM lakehouse.bronze.ecommerce_events.snapshots\")\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "# Get first snapshot ID for time travel\n",
    "first_snapshot = snapshots_df.orderBy(\"committed_at\").first()[\"snapshot_id\"]\n",
    "print(f\"First snapshot ID: {first_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "time-travel-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA AT FIRST SNAPSHOT (before new column) ===\n",
      "+-----------------+\n",
      "|count_at_snapshot|\n",
      "+-----------------+\n",
      "|          3533286|\n",
      "+-----------------+\n",
      "\n",
      "=== CURRENT DATA (after inserts) ===\n",
      "+-------------+\n",
      "|current_count|\n",
      "+-------------+\n",
      "|      4264754|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cột count_at_snapshot là gì\n",
    "\n",
    "# Query data at first snapshot (before schema evolution)\n",
    "print(\"=== DATA AT FIRST SNAPSHOT (before new column) ===\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count_at_snapshot   \n",
    "    FROM lakehouse.bronze.ecommerce_events \n",
    "    VERSION AS OF {first_snapshot}\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"=== CURRENT DATA (after inserts) ===\")\n",
    "spark.sql(\"SELECT COUNT(*) as current_count FROM lakehouse.bronze.ecommerce_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: GOLD LAYER - Aggregated Tables\n",
    "\n",
    "Create business-level aggregations with **partitioning** and **sorting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gold-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 19.07 giây\n",
      "Gold: daily_sales_summary created!\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "|event_date|total_events|views|add_to_cart|purchases| revenue|unique_users|conversion_rate|      _aggregated_at|\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "|2019-10-03|      124847|56934|      35745|     8865|43380.98|       16323|          15.57|2026-02-03 09:49:...|\n",
      "|2019-10-04|      115612|53674|      31347|     7562|35887.15|       14732|          14.09|2026-02-03 09:49:...|\n",
      "|2019-10-01|      142414|61209|      46916|     8476|43497.17|       19230|          13.85|2026-02-03 09:49:...|\n",
      "|2019-10-02|      201068|76497|      89124|     9100|45746.20|       33859|          11.90|2026-02-03 09:49:...|\n",
      "|2019-10-07|      181451|75899|      66954|     9376|46837.58|       26427|          12.35|2026-02-03 09:49:...|\n",
      "|2019-10-08|      148944|64948|      47364|     8604|43015.61|       20140|          13.25|2026-02-03 09:49:...|\n",
      "|2019-10-05|      106343|49411|      31201|     5940|29228.55|       14990|          12.02|2026-02-03 09:49:...|\n",
      "|2019-10-06|      187383|72916|      81376|     7265|33167.80|       31439|           9.96|2026-02-03 09:49:...|\n",
      "|2019-10-11|      119195|57104|      31451|     6922|36024.56|       16504|          12.12|2026-02-03 09:49:...|\n",
      "|2019-10-12|      111166|52637|      30480|     6431|32269.39|       14986|          12.22|2026-02-03 09:49:...|\n",
      "|2019-10-09|      139901|65433|      37991|     8464|44051.02|       18174|          12.94|2026-02-03 09:49:...|\n",
      "|2019-10-10|      132634|60949|      36620|     8117|42017.69|       17127|          13.32|2026-02-03 09:49:...|\n",
      "|2019-10-15|      130030|59861|      37521|     8299|40471.08|       16950|          13.86|2026-02-03 09:49:...|\n",
      "|2019-10-16|      136043|63004|      38408|     9096|42755.28|       17488|          14.44|2026-02-03 09:49:...|\n",
      "|2019-10-13|      116755|53938|      33367|     7088|35686.30|       15627|          13.14|2026-02-03 09:49:...|\n",
      "|2019-10-14|      133258|62188|      37722|     8004|40785.22|       17377|          12.87|2026-02-03 09:49:...|\n",
      "|2019-10-19|      104300|49687|      27626|     6064|29598.43|       14533|          12.20|2026-02-03 09:49:...|\n",
      "|2019-10-20|      114682|52728|      31843|     6836|34260.94|       15658|          12.96|2026-02-03 09:49:...|\n",
      "|2019-10-17|      133870|61975|      37136|     9236|45325.16|       17200|          14.90|2026-02-03 09:49:...|\n",
      "|2019-10-18|      119073|57520|      31044|     7920|39829.67|       15689|          13.77|2026-02-03 09:49:...|\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Gold 1: Daily Sales Summary (with partitioning)\n",
    "df_daily = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'cart' THEN 1 END) AS add_to_cart,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        COUNT(DISTINCT user_id) AS unique_users,\n",
    "        ROUND(COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) * 100.0 / \n",
    "              NULLIF(COUNT(CASE WHEN event_type = 'view' THEN 1 END), 0), 2) AS conversion_rate,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    GROUP BY event_date\n",
    "    ORDER BY event_date\n",
    "\"\"\")\n",
    "\n",
    "df_daily.writeTo(\"lakehouse.gold.daily_sales_summary\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "\n",
    "print(\"Gold: daily_sales_summary created!\")\n",
    "spark.table(\"lakehouse.gold.daily_sales_summary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gold-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 9.97 giây\n",
      "Gold: brand_performance created!\n",
      "+--------+------------+------+---------+---------+---------------+----------------+--------------------+\n",
      "|   brand|total_events| views|purchases|  revenue|avg_order_value|unique_customers|      _aggregated_at|\n",
      "+--------+------------+------+---------+---------+---------------+----------------+--------------------+\n",
      "|  runail|     1528908|608857|   111408|343433.19|           3.08|          227164|2026-02-03 09:50:...|\n",
      "| grattol|      852591|426257|    49542|266295.94|           5.38|          123453|2026-02-03 09:50:...|\n",
      "|   irisk|     1033852|409737|    73806|223903.38|           3.03|          187621|2026-02-03 09:50:...|\n",
      "|     uno|      250377|121177|    17586|190719.46|          10.84|           63592|2026-02-03 09:50:...|\n",
      "|  strong|       60713| 50925|      850|151941.80|         178.76|           24736|2026-02-03 09:50:...|\n",
      "|  masura|      861763|342247|    49751|139764.86|           2.81|           97435|2026-02-03 09:50:...|\n",
      "|jessnail|      252996|172238|     9661|134775.04|          13.95|           84923|2026-02-03 09:50:...|\n",
      "|ingarden|      430958|175637|    27411|124606.04|           4.55|           69797|2026-02-03 09:50:...|\n",
      "|   estel|      360912|206045|    19438|121788.76|           6.27|          101698|2026-02-03 09:50:...|\n",
      "|     cnd|      177662|107510|     7686|106327.80|          13.83|           40770|2026-02-03 09:50:...|\n",
      "+--------+------------+------+---------+---------+---------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Gold 2: Brand Performance (sorted for efficient queries)\n",
    "df_brand = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        brand,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        CAST(AVG(CASE WHEN event_type = 'purchase' THEN price END) AS DECIMAL(10,2)) AS avg_order_value,\n",
    "        COUNT(DISTINCT user_id) AS unique_customers,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    WHERE brand != 'unknown'\n",
    "    GROUP BY brand\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Write with sort order for optimized queries\n",
    "df_brand.writeTo(\"lakehouse.gold.brand_performance\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.distribution-mode\", \"hash\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "\n",
    "print(\"Gold: brand_performance created!\")\n",
    "spark.table(\"lakehouse.gold.brand_performance\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gold-hourly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 5.28 giây\n",
      "Gold: hourly_traffic created!\n",
      "+----------+------------+------+---------+---------+--------------------+\n",
      "|event_hour|total_events| views|purchases|  revenue|      _aggregated_at|\n",
      "+----------+------------+------+---------+---------+--------------------+\n",
      "|         0|      202824| 89823|    11857| 55647.47|2026-02-03 09:50:...|\n",
      "|         1|      184093| 76937|    11941| 53548.94|2026-02-03 09:50:...|\n",
      "|         2|      202669| 84941|    12306| 58054.87|2026-02-03 09:50:...|\n",
      "|         3|      283222|122096|    15500| 75796.41|2026-02-03 09:50:...|\n",
      "|         4|      398557|181733|    22023|108355.57|2026-02-03 09:50:...|\n",
      "|         5|      603889|273928|    35310|172329.76|2026-02-03 09:50:...|\n",
      "|         6|      814449|377012|    50802|255409.26|2026-02-03 09:50:...|\n",
      "|         7|      968572|455206|    62334|311343.85|2026-02-03 09:50:...|\n",
      "|         8|     1066729|502937|    70323|362003.61|2026-02-03 09:50:...|\n",
      "|         9|     1133492|529162|    76504|385431.64|2026-02-03 09:50:...|\n",
      "|        10|     1189988|558185|    79382|405974.49|2026-02-03 09:50:...|\n",
      "|        11|     1230501|574715|    85609|435985.12|2026-02-03 09:50:...|\n",
      "|        12|     1244408|583202|    83755|420905.90|2026-02-03 09:50:...|\n",
      "|        13|     1176683|559653|    77492|385299.43|2026-02-03 09:50:...|\n",
      "|        14|     1107535|525131|    71418|350829.13|2026-02-03 09:50:...|\n",
      "|        15|     1065422|503031|    66838|332991.73|2026-02-03 09:50:...|\n",
      "|        16|     1105075|516846|    65712|325281.98|2026-02-03 09:50:...|\n",
      "|        17|     1179788|561015|    67888|331105.53|2026-02-03 09:50:...|\n",
      "|        18|     1302034|606853|    73631|356059.66|2026-02-03 09:50:...|\n",
      "|        19|     1363549|641355|    76563|367029.08|2026-02-03 09:50:...|\n",
      "|        20|     1226341|574704|    72888|348477.12|2026-02-03 09:50:...|\n",
      "|        21|      845428|389034|    49831|235477.64|2026-02-03 09:50:...|\n",
      "|        22|      505018|234253|    29863|136477.96|2026-02-03 09:50:...|\n",
      "|        23|      292574|136069|    17237| 78188.72|2026-02-03 09:50:...|\n",
      "+----------+------------+------+---------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Gold 3: Hourly Traffic Pattern\n",
    "df_hourly = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_hour,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    GROUP BY event_hour\n",
    "    ORDER BY event_hour\n",
    "\"\"\")\n",
    "\n",
    "df_hourly.writeTo(\"lakehouse.gold.hourly_traffic\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "\n",
    "print(\"Gold: hourly_traffic created!\")\n",
    "spark.table(\"lakehouse.gold.hourly_traffic\").show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gold-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Thời gian chạy: 12.31 giây\n",
      "Gold: daily_sales_by_category created!\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "|event_date|       category_code|total_events|purchases|revenue|      _aggregated_at|\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "|2019-10-03|appliances.enviro...|         913|       32|1204.67|2026-02-03 09:50:...|\n",
      "|2019-10-03|       apparel.glove|         196|       24| 179.69|2026-02-03 09:50:...|\n",
      "|2019-10-03|furniture.bathroo...|         125|        7| 172.64|2026-02-03 09:50:...|\n",
      "|2019-10-03|     accessories.bag|         153|        3| 127.38|2026-02-03 09:50:...|\n",
      "|2019-10-03| stationery.cartrige|         497|       42|  94.52|2026-02-03 09:50:...|\n",
      "|2019-10-03|furniture.living_...|         234|        1|  82.54|2026-02-03 09:50:...|\n",
      "|2019-10-03|accessories.cosme...|          63|        5|  27.80|2026-02-03 09:50:...|\n",
      "|2019-10-03|furniture.living_...|           3|        0|   0.00|2026-02-03 09:50:...|\n",
      "|2019-10-03|appliances.person...|          18|        0|   0.00|2026-02-03 09:50:...|\n",
      "|2019-10-04|appliances.enviro...|         881|       32| 858.49|2026-02-03 09:50:...|\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Gold 4: Daily Sales by Category\n",
    "df_category = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        category_code,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    WHERE category_code != 'uncategorized'\n",
    "    GROUP BY event_date, category_code\n",
    "    ORDER BY event_date, revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "df_category.writeTo(\"lakehouse.gold.daily_sales_by_category\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Thời gian chạy: {end - start:.2f} giây\")\n",
    "\n",
    "print(\"Gold: daily_sales_by_category created!\")\n",
    "spark.table(\"lakehouse.gold.daily_sales_by_category\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4.1: Verify All Tables & Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "verify-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BRONZE TABLES\n",
      "============================================================\n",
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|   bronze|ecommerce_events|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n",
      "============================================================\n",
      "SILVER TABLES\n",
      "============================================================\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|   silver|ecommerce_events_...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "============================================================\n",
      "GOLD TABLES\n",
      "============================================================\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|     gold|   brand_performance|      false|\n",
      "|     gold|daily_sales_by_ca...|      false|\n",
      "|     gold| daily_sales_summary|      false|\n",
      "|     gold|      hourly_traffic|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BRONZE TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.bronze\").show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SILVER TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.silver\").show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GOLD TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.gold\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "verify-locations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Locations in MinIO:\n",
      "================================================================================\n",
      "lakehouse.bronze.ecommerce_events\n",
      "  -> s3://lakehouse/bronze/ecommerce_events\n",
      "lakehouse.silver.ecommerce_events_cleaned\n",
      "  -> s3://lakehouse/silver/ecommerce_events_cleaned\n",
      "lakehouse.gold.daily_sales_summary\n",
      "  -> s3://lakehouse/gold/daily_sales_summary\n",
      "lakehouse.gold.brand_performance\n",
      "  -> s3://lakehouse/gold/brand_performance\n",
      "lakehouse.gold.hourly_traffic\n",
      "  -> s3://lakehouse/gold/hourly_traffic\n",
      "lakehouse.gold.daily_sales_by_category\n",
      "  -> s3://lakehouse/gold/daily_sales_by_category\n"
     ]
    }
   ],
   "source": [
    "# Check file locations in MinIO\n",
    "tables = [\n",
    "    \"lakehouse.bronze.ecommerce_events\",\n",
    "    \"lakehouse.silver.ecommerce_events_cleaned\",\n",
    "    \"lakehouse.gold.daily_sales_summary\",\n",
    "    \"lakehouse.gold.brand_performance\",\n",
    "    \"lakehouse.gold.hourly_traffic\",\n",
    "    \"lakehouse.gold.daily_sales_by_category\"\n",
    "]\n",
    "\n",
    "print(\"Table Locations in MinIO:\")\n",
    "print(\"=\" * 80)\n",
    "for table in tables:\n",
    "    try:\n",
    "        location = spark.sql(f\"DESCRIBE EXTENDED {table}\") \\\n",
    "            .filter(col(\"col_name\") == \"Location\") \\\n",
    "            .select(\"data_type\").collect()[0][0]\n",
    "        print(f\"{table}\")\n",
    "        print(f\"  -> {location}\")\n",
    "    except:\n",
    "        print(f\"{table} - Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5: CLICKHOUSE Integration (Zero-Copy)\n",
    "\n",
    "### Run these commands to exec ClickHouse:\n",
    "```bash\n",
    "docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
    "```\n",
    "After that, run code  below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clickhouse-commands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- ================================================================\n",
      "-- CLICKHOUSE COMMANDS - Run in clickhouse-client\n",
      "-- docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
      "-- ================================================================\n",
      "\n",
      "-- 1. Create database\n",
      "CREATE DATABASE IF NOT EXISTS lakehouse;\n",
      "\n",
      "-- 2. Daily Sales Summary (Zero-Copy from MinIO)\n",
      "CREATE OR REPLACE TABLE lakehouse.daily_sales_summary\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_summary/data/*/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "-- 3. Brand Performance\n",
      "CREATE OR REPLACE TABLE lakehouse.brand_performance\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/brand_performance/data/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "-- 4. Hourly Traffic\n",
      "CREATE OR REPLACE TABLE lakehouse.hourly_traffic\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/hourly_traffic/data/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "-- 5. Daily Sales by Category\n",
      "CREATE OR REPLACE TABLE lakehouse.daily_sales_by_category\n",
      "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_by_category/data/*/*.parquet',\n",
      "             'admin', 'password', 'Parquet');\n",
      "\n",
      "\n",
      "-- ================================================================\n",
      "-- VERIFY DATA\n",
      "-- ================================================================\n",
      "\n",
      "SELECT * FROM lakehouse.daily_sales_summary;\n",
      "SELECT * FROM lakehouse.brand_performance LIMIT 10;\n",
      "SELECT * FROM lakehouse.hourly_traffic ORDER BY event_hour;\n",
      "\n",
      "-- ================================================================\n",
      "-- SAMPLE ANALYTICS QUERIES\n",
      "-- ================================================================\n",
      "\n",
      "-- Top 10 brands by revenue\n",
      "SELECT \n",
      "    brand,\n",
      "    revenue,\n",
      "    purchases,\n",
      "    round(revenue / purchases, 2) as avg_order_value\n",
      "FROM lakehouse.brand_performance\n",
      "WHERE purchases > 0\n",
      "ORDER BY revenue DESC\n",
      "LIMIT 10;\n",
      "\n",
      "-- Conversion funnel\n",
      "SELECT\n",
      "    sum(views) as total_views,\n",
      "    sum(add_to_cart) as total_carts,\n",
      "    sum(purchases) as total_purchases,\n",
      "    round(sum(purchases) * 100.0 / sum(views), 2) as overall_conversion\n",
      "FROM lakehouse.daily_sales_summary;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickhouse_sql = \"\"\"\n",
    "-- ================================================================\n",
    "-- CLICKHOUSE COMMANDS - Run in clickhouse-client\n",
    "-- docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
    "-- ================================================================\n",
    "\n",
    "-- 1. Create database\n",
    "CREATE DATABASE IF NOT EXISTS lakehouse;\n",
    "\n",
    "-- 2. Daily Sales Summary (Zero-Copy from MinIO)\n",
    "CREATE OR REPLACE TABLE lakehouse.daily_sales_summary\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_summary/data/*/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "-- 3. Brand Performance\n",
    "CREATE OR REPLACE TABLE lakehouse.brand_performance\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/brand_performance/data/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "-- 4. Hourly Traffic\n",
    "CREATE OR REPLACE TABLE lakehouse.hourly_traffic\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/hourly_traffic/data/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "-- 5. Daily Sales by Category\n",
    "CREATE OR REPLACE TABLE lakehouse.daily_sales_by_category\n",
    "ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_by_category/data/*/*.parquet',\n",
    "             'admin', 'password', 'Parquet');\n",
    "\n",
    "\n",
    "-- ================================================================\n",
    "-- VERIFY DATA\n",
    "-- ================================================================\n",
    "\n",
    "SELECT * FROM lakehouse.daily_sales_summary;\n",
    "SELECT * FROM lakehouse.brand_performance LIMIT 10;\n",
    "SELECT * FROM lakehouse.hourly_traffic ORDER BY event_hour;\n",
    "\n",
    "-- ================================================================\n",
    "-- SAMPLE ANALYTICS QUERIES\n",
    "-- ================================================================\n",
    "\n",
    "-- Top 10 brands by revenue\n",
    "SELECT \n",
    "    brand,\n",
    "    revenue,\n",
    "    purchases,\n",
    "    round(revenue / purchases, 2) as avg_order_value\n",
    "FROM lakehouse.brand_performance\n",
    "WHERE purchases > 0\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Conversion funnel\n",
    "SELECT\n",
    "    sum(views) as total_views,\n",
    "    sum(add_to_cart) as total_carts,\n",
    "    sum(purchases) as total_purchases,\n",
    "    round(sum(purchases) * 100.0 / sum(views), 2) as overall_conversion\n",
    "FROM lakehouse.daily_sales_summary;\n",
    "\"\"\"\n",
    "\n",
    "print(clickhouse_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superset-header",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## STEP 6: SUPERSET Dashboard Setup\n",
    "\n",
    "### Access\n",
    "- URL: http://localhost:8088\n",
    "- Username: `admin`\n",
    "- Password: `admin`\n",
    "\n",
    "### Add ClickHouse Connection\n",
    "1. Settings -> Database Connections -> + Database\n",
    "2. Select: **ClickHouse Connect**\n",
    "3. Connection :\n",
    "```\n",
    "-Host: clickhouse\n",
    "\n",
    "-Port: 8123\n",
    "\n",
    "-Database name: lakehouse\n",
    "\n",
    "-Username: admin\n",
    "\n",
    "-Password: password\n",
    "\n",
    "-Display Name: ClickHouse Lakehouse\n",
    "\n",
    "-SSL: (optional)\n",
    "\n",
    "click Connect/Test Connection.\n",
    "```\n",
    "\n",
    "### Create 3 Required Charts\n",
    "\n",
    "| Chart | Type | Dataset | Metrics |\n",
    "|-------|------|---------|--------|\n",
    "| Daily Revenue Trend | Line Chart | daily_sales_summary | revenue by event_date |\n",
    "| Top Brands | Bar Chart | brand_performance | revenue by brand |\n",
    "| Hourly Traffic | Area Chart | hourly_traffic | views, purchases by event_hour |\n",
    "\n",
    "### Create Dashboard\n",
    "1. Dashboards -> + Dashboard\n",
    "2. Add the 3 charts\n",
    "3. Arrange and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbt-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 7: dbt Transformation (Alternative)\n",
    "\n",
    "### Run dbt commands:\n",
    "```bash\n",
    "# Enter dbt container\n",
    "docker exec -it lakehouse-dbt bash\n",
    "\n",
    "# Run transformations\n",
    "cd /usr/app/dbt/lakehouse_dbt\n",
    "dbt debug\n",
    "dbt run\n",
    "```\n",
    "\n",
    "dbt models are located in:\n",
    "- `/dbt/lakehouse_dbt/models/silver/` - Silver layer transformations\n",
    "- `/dbt/lakehouse_dbt/models/gold/` - Gold layer aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e12560-fa85-4850-bc23-d65b57018dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
