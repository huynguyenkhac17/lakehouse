{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# FULLSTACK OPEN-SOURCE LAKEHOUSE PLATFORM\n",
    "\n",
    "## E-Commerce Event History Analysis (Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u eCommerce Events History in Cosmetics Shop trÃªn Kanggle)\n",
    "\n",
    "### Architecture Stack\n",
    "| Layer | Technology | Port |\n",
    "|-------|------------|------|\n",
    "| Storage | MinIO | 9000, 9001 |\n",
    "| Table Format | Apache Iceberg | - |\n",
    "| Catalog | Iceberg REST | 8181 |\n",
    "| Compute | Apache Spark | 8080, 8888 |\n",
    "| Transformation | dbt | - |\n",
    "| Serving | ClickHouse | 8123, 9440 |\n",
    "| Visualization | Apache Superset | 8088 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/04 16:35:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/04 16:35:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…Started new sparksession.\n",
      "Spark Version: 3.5.1\n",
      "Catalog: lakehouse\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"âœ… Stop previous SparkSession.\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lakehouse ETL eCommerces data\") \\\n",
    "    .getOrCreate()\n",
    "print(\"âœ…Started new sparksession.\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "namespaces",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Medallion namespaces\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse.gold\")\n",
    "\n",
    "spark.sql(\"SHOW NAMESPACES IN lakehouse\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: BRONZE LAYER - Raw Data Ingestion\n",
    "\n",
    "- Read CSV raw data\n",
    "- Add metadata columns: `_ingestion_time`, `_source_file`\n",
    "- Write as Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bronze-read",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 27.29 giÃ¢y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 20,692,840\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+------------------------------------+\n",
      "|event_time         |event_type|product_id|category_id        |category_code|brand |price|user_id  |user_session                        |\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+------------------------------------+\n",
      "|2019-10-01 00:00:00|cart      |5773203   |1487580005134238553|NULL         |runail|2.62 |463240011|26dd6e6e-4dac-4778-8d2c-92e149dab885|\n",
      "|2019-10-01 00:00:03|cart      |5773353   |1487580005134238553|NULL         |runail|2.62 |463240011|26dd6e6e-4dac-4778-8d2c-92e149dab885|\n",
      "|2019-10-01 00:00:07|cart      |5881589   |2151191071051219817|NULL         |lovely|13.48|429681830|49e8d843-adf3-428b-a2c3-fe8bc6a307c9|\n",
      "|2019-10-01 00:00:07|cart      |5723490   |1487580005134238553|NULL         |runail|2.62 |463240011|26dd6e6e-4dac-4778-8d2c-92e149dab885|\n",
      "|2019-10-01 00:00:15|cart      |5881449   |1487580013522845895|NULL         |lovely|0.56 |429681830|49e8d843-adf3-428b-a2c3-fe8bc6a307c9|\n",
      "+-------------------+----------+----------+-------------------+-------------+------+-----+---------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_19_oct = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Oct.csv\")\n",
    "df_19_nov = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Nov.csv\")\n",
    "df_19_dec = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2019-Dec.csv\")\n",
    "df_20_jan = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2020-Jan.csv\")\n",
    "df_20_feb = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/notebooks/archive/2020-Feb.csv\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "\n",
    "df_raw = df_19_oct.union(df_19_nov).union(df_19_dec).union(df_20_jan).union(df_20_feb)\n",
    "\n",
    "print(f\"Total records: {df_raw.count():,}\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c162863e-70e6-40c2-8b69-09bb8d959876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 32.05 giÃ¢y\n",
      "BRONZE layer created!\n",
      "+--------+\n",
      "|   total|\n",
      "+--------+\n",
      "|20692840|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add metadata columns\n",
    "df_19_oct = df_19_oct \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2019-Oct.csv\"))\n",
    "df_19_nov = df_19_nov \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2019-Nov.csv\"))\n",
    "df_19_dec = df_19_dec \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2019-Dec.csv\"))\n",
    "df_20_jan = df_20_jan \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2020-Jan.csv\"))\n",
    "df_20_feb = df_20_feb \\\n",
    "    .withColumn(\"_ingestion_time\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(\"2020-Feb.csv\"))\n",
    "\n",
    "df_bronze = df_19_oct.union(df_19_nov).union(df_19_dec).union(df_20_jan).union(df_20_feb)\n",
    "\n",
    "start = time.time()\n",
    "# Write to Bronze layer\n",
    "df_bronze.writeTo(\"lakehouse.bronze.ecommerce_events\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "\n",
    "print(\"BRONZE layer created!\")\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM lakehouse.bronze.ecommerce_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: SILVER LAYER - Cleaned Data with Partitioning\n",
    "\n",
    "- Parse timestamps\n",
    "- Handle NULL values\n",
    "- **Partition by event_date** (optimize query performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "silver-transform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 0.12 giÃ¢y\n",
      "root\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = false)\n",
      " |-- brand: string (nullable = false)\n",
      " |-- price: decimal(10,2) (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      " |-- _processed_at: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Transform Bronze -> Silver\n",
    "df_silver = spark.table(\"lakehouse.bronze.ecommerce_events\") \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss 'UTC'\")) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"brand\", coalesce(col(\"brand\"), lit(\"unknown\"))) \\\n",
    "    .withColumn(\"category_code\", coalesce(col(\"category_code\"), lit(\"uncategorized\"))) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"decimal(10,2)\")) \\\n",
    "    .withColumn(\"_processed_at\", current_timestamp()) \\\n",
    "    .drop(\"event_time\", \"_ingestion_time\", \"_source_file\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "silver-write",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 17.95 giÃ¢y\n",
      "SILVER layer created with partitioning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:============================================>           (30 + 4) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_date|\n",
      "+----------+\n",
      "|2019-10-01|\n",
      "|2019-10-02|\n",
      "|2019-10-03|\n",
      "|2019-10-04|\n",
      "|2019-10-05|\n",
      "|2019-10-06|\n",
      "|2019-10-07|\n",
      "|2019-10-08|\n",
      "|2019-10-09|\n",
      "|2019-10-10|\n",
      "|2019-10-11|\n",
      "|2019-10-12|\n",
      "|2019-10-13|\n",
      "|2019-10-14|\n",
      "|2019-10-15|\n",
      "|2019-10-16|\n",
      "|2019-10-17|\n",
      "|2019-10-18|\n",
      "|2019-10-19|\n",
      "|2019-10-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Write with PARTITIONING by date\n",
    "df_silver.writeTo(\"lakehouse.silver.ecommerce_events_cleaned\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "print(\"SILVER layer created with partitioning!\")\n",
    "\n",
    "# Verify partitions\n",
    "spark.sql(\"SELECT DISTINCT event_date FROM lakehouse.silver.ecommerce_events_cleaned ORDER BY event_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: GOLD LAYER - Aggregated Tables\n",
    "\n",
    "Create business-level aggregations with **partitioning** and **sorting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gold-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 6.34 giÃ¢y\n",
      "Gold: daily_sales_summary created!\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "|event_date|total_events|views|add_to_cart|purchases| revenue|unique_users|conversion_rate|      _aggregated_at|\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "|2019-10-03|      124847|56934|      35745|     8865|43380.98|       16323|          15.57|2026-02-04 16:37:...|\n",
      "|2019-10-04|      115612|53674|      31347|     7562|35887.15|       14732|          14.09|2026-02-04 16:37:...|\n",
      "|2019-10-01|      142414|61209|      46916|     8476|43497.17|       19230|          13.85|2026-02-04 16:37:...|\n",
      "|2019-10-02|      201068|76497|      89124|     9100|45746.20|       33859|          11.90|2026-02-04 16:37:...|\n",
      "|2019-10-07|      181451|75899|      66954|     9376|46837.58|       26427|          12.35|2026-02-04 16:37:...|\n",
      "|2019-10-08|      148944|64948|      47364|     8604|43015.61|       20140|          13.25|2026-02-04 16:37:...|\n",
      "|2019-10-05|      106343|49411|      31201|     5940|29228.55|       14990|          12.02|2026-02-04 16:37:...|\n",
      "|2019-10-06|      187383|72916|      81376|     7265|33167.80|       31439|           9.96|2026-02-04 16:37:...|\n",
      "|2019-10-11|      119195|57104|      31451|     6922|36024.56|       16504|          12.12|2026-02-04 16:37:...|\n",
      "|2019-10-12|      111166|52637|      30480|     6431|32269.39|       14986|          12.22|2026-02-04 16:37:...|\n",
      "|2019-10-09|      139901|65433|      37991|     8464|44051.02|       18174|          12.94|2026-02-04 16:37:...|\n",
      "|2019-10-10|      132634|60949|      36620|     8117|42017.69|       17127|          13.32|2026-02-04 16:37:...|\n",
      "|2019-10-15|      130030|59861|      37521|     8299|40471.08|       16950|          13.86|2026-02-04 16:37:...|\n",
      "|2019-10-16|      136043|63004|      38408|     9096|42755.28|       17488|          14.44|2026-02-04 16:37:...|\n",
      "|2019-10-13|      116755|53938|      33367|     7088|35686.30|       15627|          13.14|2026-02-04 16:37:...|\n",
      "|2019-10-14|      133258|62188|      37722|     8004|40785.22|       17377|          12.87|2026-02-04 16:37:...|\n",
      "|2019-10-19|      104300|49687|      27626|     6064|29598.43|       14533|          12.20|2026-02-04 16:37:...|\n",
      "|2019-10-20|      114682|52728|      31843|     6836|34260.94|       15658|          12.96|2026-02-04 16:37:...|\n",
      "|2019-10-17|      133870|61975|      37136|     9236|45325.16|       17200|          14.90|2026-02-04 16:37:...|\n",
      "|2019-10-18|      119073|57520|      31044|     7920|39829.67|       15689|          13.77|2026-02-04 16:37:...|\n",
      "+----------+------------+-----+-----------+---------+--------+------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Gold 1: Daily Sales Summary (with partitioning)\n",
    "df_daily = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'cart' THEN 1 END) AS add_to_cart,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        COUNT(DISTINCT user_id) AS unique_users,\n",
    "        ROUND(COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) * 100.0 / \n",
    "              NULLIF(COUNT(CASE WHEN event_type = 'view' THEN 1 END), 0), 2) AS conversion_rate,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    GROUP BY event_date\n",
    "    ORDER BY event_date\n",
    "\"\"\")\n",
    "\n",
    "df_daily.writeTo(\"lakehouse.gold.daily_sales_summary\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "\n",
    "print(\"Gold: daily_sales_summary created!\")\n",
    "spark.table(\"lakehouse.gold.daily_sales_summary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gold-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 4.36 giÃ¢y\n",
      "Gold: brand_performance created!\n",
      "+--------+------------+------+---------+---------+---------------+----------------+--------------------+\n",
      "|   brand|total_events| views|purchases|  revenue|avg_order_value|unique_customers|      _aggregated_at|\n",
      "+--------+------------+------+---------+---------+---------------+----------------+--------------------+\n",
      "|  runail|     1528908|608857|   111408|343433.19|           3.08|          227164|2026-02-04 16:37:...|\n",
      "| grattol|      852591|426257|    49542|266295.94|           5.38|          123453|2026-02-04 16:37:...|\n",
      "|   irisk|     1033852|409737|    73806|223903.38|           3.03|          187621|2026-02-04 16:37:...|\n",
      "|     uno|      250377|121177|    17586|190719.46|          10.84|           63592|2026-02-04 16:37:...|\n",
      "|  strong|       60713| 50925|      850|151941.80|         178.76|           24736|2026-02-04 16:37:...|\n",
      "|  masura|      861763|342247|    49751|139764.86|           2.81|           97435|2026-02-04 16:37:...|\n",
      "|jessnail|      252996|172238|     9661|134775.04|          13.95|           84923|2026-02-04 16:37:...|\n",
      "|ingarden|      430958|175637|    27411|124606.04|           4.55|           69797|2026-02-04 16:37:...|\n",
      "|   estel|      360912|206045|    19438|121788.76|           6.27|          101698|2026-02-04 16:37:...|\n",
      "|     cnd|      177662|107510|     7686|106327.80|          13.83|           40770|2026-02-04 16:37:...|\n",
      "+--------+------------+------+---------+---------+---------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Gold 2: Brand Performance (sorted for efficient queries)\n",
    "df_brand = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        brand,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        CAST(AVG(CASE WHEN event_type = 'purchase' THEN price END) AS DECIMAL(10,2)) AS avg_order_value,\n",
    "        COUNT(DISTINCT user_id) AS unique_customers,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    WHERE brand != 'unknown'\n",
    "    GROUP BY brand\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Write with sort order for optimized queries\n",
    "df_brand.writeTo(\"lakehouse.gold.brand_performance\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.distribution-mode\", \"hash\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "\n",
    "print(\"Gold: brand_performance created!\")\n",
    "spark.table(\"lakehouse.gold.brand_performance\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gold-hourly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 2.26 giÃ¢y\n",
      "Gold: hourly_traffic created!\n",
      "+----------+------------+------+---------+---------+--------------------+\n",
      "|event_hour|total_events| views|purchases|  revenue|      _aggregated_at|\n",
      "+----------+------------+------+---------+---------+--------------------+\n",
      "|         0|      202824| 89823|    11857| 55647.47|2026-02-04 16:37:...|\n",
      "|         1|      184093| 76937|    11941| 53548.94|2026-02-04 16:37:...|\n",
      "|         2|      202669| 84941|    12306| 58054.87|2026-02-04 16:37:...|\n",
      "|         3|      283222|122096|    15500| 75796.41|2026-02-04 16:37:...|\n",
      "|         4|      398557|181733|    22023|108355.57|2026-02-04 16:37:...|\n",
      "|         5|      603889|273928|    35310|172329.76|2026-02-04 16:37:...|\n",
      "|         6|      814449|377012|    50802|255409.26|2026-02-04 16:37:...|\n",
      "|         7|      968572|455206|    62334|311343.85|2026-02-04 16:37:...|\n",
      "|         8|     1066729|502937|    70323|362003.61|2026-02-04 16:37:...|\n",
      "|         9|     1133492|529162|    76504|385431.64|2026-02-04 16:37:...|\n",
      "|        10|     1189988|558185|    79382|405974.49|2026-02-04 16:37:...|\n",
      "|        11|     1230501|574715|    85609|435985.12|2026-02-04 16:37:...|\n",
      "|        12|     1244408|583202|    83755|420905.90|2026-02-04 16:37:...|\n",
      "|        13|     1176683|559653|    77492|385299.43|2026-02-04 16:37:...|\n",
      "|        14|     1107535|525131|    71418|350829.13|2026-02-04 16:37:...|\n",
      "|        15|     1065422|503031|    66838|332991.73|2026-02-04 16:37:...|\n",
      "|        16|     1105075|516846|    65712|325281.98|2026-02-04 16:37:...|\n",
      "|        17|     1179788|561015|    67888|331105.53|2026-02-04 16:37:...|\n",
      "|        18|     1302034|606853|    73631|356059.66|2026-02-04 16:37:...|\n",
      "|        19|     1363549|641355|    76563|367029.08|2026-02-04 16:37:...|\n",
      "|        20|     1226341|574704|    72888|348477.12|2026-02-04 16:37:...|\n",
      "|        21|      845428|389034|    49831|235477.64|2026-02-04 16:37:...|\n",
      "|        22|      505018|234253|    29863|136477.96|2026-02-04 16:37:...|\n",
      "|        23|      292574|136069|    17237| 78188.72|2026-02-04 16:37:...|\n",
      "+----------+------------+------+---------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Gold 3: Hourly Traffic Pattern\n",
    "df_hourly = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_hour,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS views,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    GROUP BY event_hour\n",
    "    ORDER BY event_hour\n",
    "\"\"\")\n",
    "\n",
    "df_hourly.writeTo(\"lakehouse.gold.hourly_traffic\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "\n",
    "print(\"Gold: hourly_traffic created!\")\n",
    "spark.table(\"lakehouse.gold.hourly_traffic\").show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gold-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Thá»i gian cháº¡y: 3.18 giÃ¢y\n",
      "Gold: daily_sales_by_category created!\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "|event_date|       category_code|total_events|purchases|revenue|      _aggregated_at|\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "|2019-10-03|appliances.enviro...|         913|       32|1204.67|2026-02-04 16:37:...|\n",
      "|2019-10-03|       apparel.glove|         196|       24| 179.69|2026-02-04 16:37:...|\n",
      "|2019-10-03|furniture.bathroo...|         125|        7| 172.64|2026-02-04 16:37:...|\n",
      "|2019-10-03|     accessories.bag|         153|        3| 127.38|2026-02-04 16:37:...|\n",
      "|2019-10-03| stationery.cartrige|         497|       42|  94.52|2026-02-04 16:37:...|\n",
      "|2019-10-03|furniture.living_...|         234|        1|  82.54|2026-02-04 16:37:...|\n",
      "|2019-10-03|accessories.cosme...|          63|        5|  27.80|2026-02-04 16:37:...|\n",
      "|2019-10-03|furniture.living_...|           3|        0|   0.00|2026-02-04 16:37:...|\n",
      "|2019-10-03|appliances.person...|          18|        0|   0.00|2026-02-04 16:37:...|\n",
      "|2019-10-04|appliances.enviro...|         881|       32| 858.49|2026-02-04 16:37:...|\n",
      "+----------+--------------------+------------+---------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Gold 4: Daily Sales by Category\n",
    "df_category = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_date,\n",
    "        category_code,\n",
    "        COUNT(*) AS total_events,\n",
    "        COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases,\n",
    "        CAST(SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS DECIMAL(12,2)) AS revenue,\n",
    "        current_timestamp() AS _aggregated_at\n",
    "    FROM lakehouse.silver.ecommerce_events_cleaned\n",
    "    WHERE category_code != 'uncategorized'\n",
    "    GROUP BY event_date, category_code\n",
    "    ORDER BY event_date, revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "df_category.writeTo(\"lakehouse.gold.daily_sales_by_category\") \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .partitionedBy(\"event_date\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"â±ï¸ Thá»i gian cháº¡y: {end - start:.2f} giÃ¢y\")\n",
    "\n",
    "print(\"Gold: daily_sales_by_category created!\")\n",
    "spark.table(\"lakehouse.gold.daily_sales_by_category\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4.1: Verify All Tables & Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "verify-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BRONZE TABLES\n",
      "============================================================\n",
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|   bronze|ecommerce_events|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n",
      "============================================================\n",
      "SILVER TABLES\n",
      "============================================================\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|   silver|ecommerce_events_...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "============================================================\n",
      "GOLD TABLES\n",
      "============================================================\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|     gold|   brand_performance|      false|\n",
      "|     gold|daily_sales_by_ca...|      false|\n",
      "|     gold| daily_sales_summary|      false|\n",
      "|     gold|      hourly_traffic|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BRONZE TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.bronze\").show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SILVER TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.silver\").show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GOLD TABLES\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SHOW TABLES IN lakehouse.gold\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "verify-locations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Locations in MinIO:\n",
      "================================================================================\n",
      "lakehouse.bronze.ecommerce_events\n",
      "  -> s3://lakehouse/bronze/ecommerce_events\n",
      "lakehouse.silver.ecommerce_events_cleaned\n",
      "  -> s3://lakehouse/silver/ecommerce_events_cleaned\n",
      "lakehouse.gold.daily_sales_summary\n",
      "  -> s3://lakehouse/gold/daily_sales_summary\n",
      "lakehouse.gold.brand_performance\n",
      "  -> s3://lakehouse/gold/brand_performance\n",
      "lakehouse.gold.hourly_traffic\n",
      "  -> s3://lakehouse/gold/hourly_traffic\n",
      "lakehouse.gold.daily_sales_by_category\n",
      "  -> s3://lakehouse/gold/daily_sales_by_category\n"
     ]
    }
   ],
   "source": [
    "# Check file locations in MinIO\n",
    "tables = [\n",
    "    \"lakehouse.bronze.ecommerce_events\",\n",
    "    \"lakehouse.silver.ecommerce_events_cleaned\",\n",
    "    \"lakehouse.gold.daily_sales_summary\",\n",
    "    \"lakehouse.gold.brand_performance\",\n",
    "    \"lakehouse.gold.hourly_traffic\",\n",
    "    \"lakehouse.gold.daily_sales_by_category\"\n",
    "]\n",
    "\n",
    "print(\"Table Locations in MinIO:\")\n",
    "print(\"=\" * 80)\n",
    "for table in tables:\n",
    "    try:\n",
    "        location = spark.sql(f\"DESCRIBE EXTENDED {table}\") \\\n",
    "            .filter(col(\"col_name\") == \"Location\") \\\n",
    "            .select(\"data_type\").collect()[0][0]\n",
    "        print(f\"{table}\")\n",
    "        print(f\"  -> {location}\")\n",
    "    except:\n",
    "        print(f\"{table} - Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5: CLICKHOUSE Integration (Zero-Copy)\n",
    "\n",
    "### Run these commands to exec ClickHouse:\n",
    "```bash\n",
    "docker exec -it lakehouse-clickhouse clickhouse-client --user admin --password password\n",
    "```\n",
    "After that, run code  below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clickhouse-commands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ chuáº©n bá»‹ 5 cÃ¢u lá»‡nh SQL cho ClickHouse\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CLICKHOUSE SQL COMMANDS\n",
    "# ================================================================\n",
    "\n",
    "clickhouse_sql_commands = [\n",
    "    # 1. Create database\n",
    "    \"CREATE DATABASE IF NOT EXISTS lakehouse\",\n",
    "    \n",
    "    # 2. Daily Sales Summary (Zero-Copy from MinIO)\n",
    "    \"\"\"CREATE OR REPLACE TABLE lakehouse.daily_sales_summary\n",
    "    ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_summary/data/*/*.parquet',\n",
    "                 'admin', 'password', 'Parquet')\"\"\",\n",
    "    \n",
    "    # 3. Brand Performance\n",
    "    \"\"\"CREATE OR REPLACE TABLE lakehouse.brand_performance\n",
    "    ENGINE = S3('http://minio:9000/lakehouse/gold/brand_performance/data/*.parquet',\n",
    "                 'admin', 'password', 'Parquet')\"\"\",\n",
    "    \n",
    "    # 4. Hourly Traffic\n",
    "    \"\"\"CREATE OR REPLACE TABLE lakehouse.hourly_traffic\n",
    "    ENGINE = S3('http://minio:9000/lakehouse/gold/hourly_traffic/data/*.parquet',\n",
    "                 'admin', 'password', 'Parquet')\"\"\",\n",
    "    \n",
    "    # 5. Daily Sales by Category\n",
    "    \"\"\"CREATE OR REPLACE TABLE lakehouse.daily_sales_by_category\n",
    "    ENGINE = S3('http://minio:9000/lakehouse/gold/daily_sales_by_category/data/*/*.parquet',\n",
    "                 'admin', 'password', 'Parquet')\"\"\"\n",
    "]\n",
    "\n",
    "print(\"âœ… ÄÃ£ chuáº©n bá»‹\", len(clickhouse_sql_commands), \"cÃ¢u lá»‡nh SQL cho ClickHouse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de5471d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Äang káº¿t ná»‘i Ä‘áº¿n ClickHouse qua HTTP API...\n",
      "âœ… Káº¿t ná»‘i thÃ nh cÃ´ng! ClickHouse version: 24.1.8.22\n",
      "\n",
      "[1/5] Äang cháº¡y: CREATE DATABASE IF NOT EXISTS lakehouse...\n",
      "âœ… HoÃ n thÃ nh!\n",
      "\n",
      "[2/5] Äang cháº¡y: CREATE OR REPLACE TABLE lakehouse.daily_sales_summ...\n",
      "âœ… HoÃ n thÃ nh!\n",
      "\n",
      "[3/5] Äang cháº¡y: CREATE OR REPLACE TABLE lakehouse.brand_performanc...\n",
      "âœ… HoÃ n thÃ nh!\n",
      "\n",
      "[4/5] Äang cháº¡y: CREATE OR REPLACE TABLE lakehouse.hourly_traffic\n",
      " ...\n",
      "âœ… HoÃ n thÃ nh!\n",
      "\n",
      "[5/5] Äang cháº¡y: CREATE OR REPLACE TABLE lakehouse.daily_sales_by_c...\n",
      "âœ… HoÃ n thÃ nh!\n",
      "daily_sales_summary: 304 rows\n",
      "brand_performance: 546 rows\n",
      "hourly_traffic: 48 rows\n",
      "daily_sales_by_category: 2950 rows\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def run_clickhouse_query(query, host='clickhouse', port=8123, user='admin', password='password'):\n",
    "    \"\"\"Cháº¡y SQL query thÃ´ng qua HTTP API cá»§a ClickHouse\"\"\"\n",
    "    url = f'http://{host}:{port}/'\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            auth=(user, password),\n",
    "            data=query.encode('utf-8'),\n",
    "            params={'database': 'default'}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Lá»—i: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    print(\"ðŸ”— Äang káº¿t ná»‘i Ä‘áº¿n ClickHouse qua HTTP API...\")\n",
    "    \n",
    "    # Test connection\n",
    "    result = run_clickhouse_query(\"SELECT version()\")\n",
    "    print(f\"âœ… Káº¿t ná»‘i thÃ nh cÃ´ng! ClickHouse version: {result.strip()}\")\n",
    "    \n",
    "    # Cháº¡y tá»«ng cÃ¢u lá»‡nh\n",
    "    for i, sql in enumerate(clickhouse_sql_commands, 1):\n",
    "        try:\n",
    "            print(f\"\\n[{i}/{len(clickhouse_sql_commands)}] Äang cháº¡y: {sql[:50]}...\")\n",
    "            run_clickhouse_query(sql)\n",
    "            print(f\"âœ… HoÃ n thÃ nh!\")\n",
    "            time.sleep(0.5)  # Delay nhá» giá»¯a cÃ¡c lá»‡nh\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Lá»—i: {str(e)}\")\n",
    "    \n",
    "    # Verify tables\n",
    "    tables = [\n",
    "        'lakehouse.daily_sales_summary',\n",
    "        'lakehouse.brand_performance',\n",
    "        'lakehouse.hourly_traffic',\n",
    "        'lakehouse.daily_sales_by_category'\n",
    "    ]\n",
    "    \n",
    "    for table in tables:\n",
    "        result = run_clickhouse_query(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        print(f\"{table.split('.')[-1]}: {result.strip()} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Lá»—i: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17fda1f-ccd6-442e-b784-cae4cef797bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## STEP 6: SUPERSET Dashboard Setup\n",
    "\n",
    "### Access\n",
    "- URL: http://localhost:8088\n",
    "- Username: `admin`\n",
    "- Password: `admin`\n",
    "\n",
    "### Add ClickHouse Connection\n",
    "1. Settings -> Database Connections -> + Database\n",
    "2. Select: **ClickHouse Connect**\n",
    "3. Connection :\n",
    "```\n",
    "-Host: clickhouse\n",
    "\n",
    "-Port: 8123\n",
    "\n",
    "-Database name: lakehouse\n",
    "\n",
    "-Username: admin\n",
    "\n",
    "-Password: password\n",
    "\n",
    "-Display Name: ClickHouse Lakehouse\n",
    "\n",
    "-SSL: (optional)\n",
    "\n",
    "click Connect/Test Connection.\n",
    "```\n",
    "\n",
    "### Create 3 Required Charts\n",
    "\n",
    "| Chart | Type | Dataset | Metrics |\n",
    "|-------|------|---------|--------|\n",
    "| Daily Revenue Trend | Line Chart | daily_sales_summary | revenue by event_date |\n",
    "| Top Brands | Bar Chart | brand_performance | revenue by brand |\n",
    "| Hourly Traffic | Area Chart | hourly_traffic | views, purchases by event_hour |\n",
    "\n",
    "### Create Dashboard\n",
    "1. Dashboards -> + Dashboard\n",
    "2. Add the 3 charts\n",
    "3. Arrange and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbt-header",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 7: dbt Transformation (Alternative)\n",
    "\n",
    "### Run dbt commands:\n",
    "```bash\n",
    "# Enter dbt container\n",
    "docker exec -it lakehouse-dbt bash\n",
    "\n",
    "# Run transformations\n",
    "cd /usr/app/dbt/lakehouse_dbt\n",
    "dbt debug\n",
    "dbt run\n",
    "```\n",
    "\n",
    "dbt models are located in:\n",
    "- `/dbt/lakehouse_dbt/models/silver/` - Silver layer transformations\n",
    "- `/dbt/lakehouse_dbt/models/gold/` - Gold layer aggregations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
