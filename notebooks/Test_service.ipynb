{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8f08ed-0ecb-4a11-979d-1af17a268665",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: SCHEMA EVOLUTION Demo\n",
    "\n",
    "Simulate adding new column `payment_method` to source data.\n",
    "Iceberg handles schema changes automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95781f28-474b-4e7d-b1c2-919e46eb20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/04 16:31:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/04 16:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/02/04 16:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Started new sparksession.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✅ Stop previous SparkSession.\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lakehouse ETL eCommerces data\") \\\n",
    "    .getOrCreate()\n",
    "print(\"✅Started new sparksession.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9846aa8-f351-4305-9837-112b9970b2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[FIELDS_ALREADY_EXISTS] Cannot add column, because `payment_method` already exists in \"STRUCT<event_time: TIMESTAMP, event_type: STRING, product_id: INT, category_id: BIGINT, category_code: STRING, brand: STRING, price: DOUBLE, user_id: INT, user_session: STRING, _ingestion_time: TIMESTAMP, _source_file: STRING, payment_method: STRING>\".; line 2 pos 4;\nAddColumns [QualifiedColType(None,payment_method,StringType,true,None,None,None)]\n+- ResolvedTable org.apache.iceberg.spark.SparkCatalog@9a49fec, bronze.ecommerce_events, lakehouse.bronze.ecommerce_events, [event_time#0, event_type#1, product_id#2, category_id#3L, category_code#4, brand#5, price#6, user_id#7, user_session#8, _ingestion_time#9, _source_file#10, payment_method#11]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add new column to Bronze table (Schema Evolution)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    ALTER TABLE lakehouse.bronze.ecommerce_events \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m    ADD COLUMN payment_method STRING\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_method\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m added!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDESCRIBE lakehouse.bronze.ecommerce_events\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [FIELDS_ALREADY_EXISTS] Cannot add column, because `payment_method` already exists in \"STRUCT<event_time: TIMESTAMP, event_type: STRING, product_id: INT, category_id: BIGINT, category_code: STRING, brand: STRING, price: DOUBLE, user_id: INT, user_session: STRING, _ingestion_time: TIMESTAMP, _source_file: STRING, payment_method: STRING>\".; line 2 pos 4;\nAddColumns [QualifiedColType(None,payment_method,StringType,true,None,None,None)]\n+- ResolvedTable org.apache.iceberg.spark.SparkCatalog@9a49fec, bronze.ecommerce_events, lakehouse.bronze.ecommerce_events, [event_time#0, event_type#1, product_id#2, category_id#3L, category_code#4, brand#5, price#6, user_id#7, user_session#8, _ingestion_time#9, _source_file#10, payment_method#11]\n"
     ]
    }
   ],
   "source": [
    "# Add new column to Bronze table (Schema Evolution)\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE lakehouse.bronze.ecommerce_events \n",
    "    ADD COLUMN payment_method STRING\n",
    "\"\"\")\n",
    "\n",
    "print(\"Column 'payment_method' added!\")\n",
    "spark.sql(\"DESCRIBE lakehouse.bronze.ecommerce_events\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d29617-6fb1-4c3f-b7e0-248f3c752ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+------+--------------+\n",
      "|event_time         |brand  |price |payment_method|\n",
      "+-------------------+-------+------+--------------+\n",
      "|2019-10-02 10:00:00|samsung|599.99|credit_card   |\n",
      "|2019-10-02 11:00:00|apple  |999.99|paypal        |\n",
      "+-------------------+-------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Insert new data with the new column (simulating Day T+1 data)\n",
    "from datetime import datetime\n",
    "\n",
    "new_data = [\n",
    "    (datetime(2019, 10, 2, 10, 0, 0), \"purchase\", 12345, 1234567890, \"electronics.phone\",\n",
    "     \"samsung\", 599.99, 100001, \"new-session-001\", datetime.now(), \"demo_day2.csv\", \"credit_card\"),\n",
    "\n",
    "    (datetime(2019, 10, 2, 11, 0, 0), \"purchase\", 12346, 1234567890, \"electronics.phone\",\n",
    "     \"apple\", 999.99, 100002, \"new-session-002\", datetime.now(), \"demo_day2.csv\", \"paypal\"),\n",
    "]\n",
    "\n",
    "\n",
    "schema = spark.table(\"lakehouse.bronze.ecommerce_events\").schema\n",
    "df_new = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "df_new.writeTo(\"lakehouse.bronze.ecommerce_events\").append()\n",
    "\n",
    "# Verify - old data has NULL for payment_method, new data has values\n",
    "spark.sql(\"\"\"\n",
    "    SELECT event_time, brand, price, payment_method \n",
    "    FROM lakehouse.bronze.ecommerce_events \n",
    "    WHERE payment_method IS NOT NULL OR brand = 'samsung'\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c49f29f-5dda-4d8c-9a73-5e08b0b9ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('event_time', TimestampType(), True), StructField('event_type', StringType(), True), StructField('product_id', IntegerType(), True), StructField('category_id', LongType(), True), StructField('category_code', StringType(), True), StructField('brand', StringType(), True), StructField('price', DoubleType(), True), StructField('user_id', IntegerType(), True), StructField('user_session', StringType(), True), StructField('_ingestion_time', TimestampType(), True), StructField('_source_file', StringType(), True), StructField('payment_method', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db67d49-42f8-4fef-97be-815feab9d41f",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: TIME TRAVEL Demo\n",
    "\n",
    "Query historical snapshots of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9dbc615-6081-4e8b-85ce-4dc9568e87ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TABLE HISTORY ===\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-30 04:10:44.685|9127348836860691094|NULL               |true               |\n",
      "|2026-01-30 04:19:26.313|6471009077848248100|9127348836860691094|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "print(\"=== TABLE HISTORY ===\")\n",
    "spark.sql(\"SELECT * FROM lakehouse.bronze.ecommerce_events.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41c8f388-4e3d-4108-a876-d7f30f2be9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SNAPSHOTS ===\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|8345581712853693656|2026-01-30 04:04:51.168|append   |\n",
      "|8961319236875262264|2026-01-30 04:08:08.245|append   |\n",
      "|7909682360207467037|2026-01-30 04:09:21.43 |append   |\n",
      "|4160572160808099697|2026-01-30 04:10:06.081|append   |\n",
      "|9127348836860691094|2026-01-30 04:10:44.685|append   |\n",
      "|6471009077848248100|2026-01-30 04:19:26.313|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "First snapshot ID: 8345581712853693656\n"
     ]
    }
   ],
   "source": [
    "# View snapshots\n",
    "print(\"=== SNAPSHOTS ===\")\n",
    "snapshots_df = spark.sql(\"SELECT snapshot_id, committed_at, operation FROM lakehouse.bronze.ecommerce_events.snapshots\")\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "# Get first snapshot ID for time travel\n",
    "first_snapshot = snapshots_df.orderBy(\"committed_at\").first()[\"snapshot_id\"]\n",
    "print(f\"First snapshot ID: {first_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40759732-dadd-46d3-a11e-2f9bb2f392f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA AT FIRST SNAPSHOT (before new column) ===\n",
      "+-----------------+\n",
      "|count_at_snapshot|\n",
      "+-----------------+\n",
      "|          3533286|\n",
      "+-----------------+\n",
      "\n",
      "=== CURRENT DATA (after inserts) ===\n",
      "+-------------+\n",
      "|current_count|\n",
      "+-------------+\n",
      "|      4264754|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cột count_at_snapshot là gì\n",
    "\n",
    "# Query data at first snapshot (before schema evolution)\n",
    "print(\"=== DATA AT FIRST SNAPSHOT (before new column) ===\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count_at_snapshot   \n",
    "    FROM lakehouse.bronze.ecommerce_events \n",
    "    VERSION AS OF {first_snapshot}\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"=== CURRENT DATA (after inserts) ===\")\n",
    "spark.sql(\"SELECT COUNT(*) as current_count FROM lakehouse.bronze.ecommerce_events\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
